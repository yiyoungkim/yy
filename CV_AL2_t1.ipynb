{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yiyoungkim/yy/blob/main/CV_AL2_t1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaJLKOJG3nTX",
        "outputId": "64389e38-96ef-4521-8adc-a5a34e7c8dd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcT_ABspzPB7"
      },
      "source": [
        "\n",
        "The goal is to solve the fine-grained image classification train the model using hyperparameter tuning, data augmentation, etc., and write a report with a detailed analysis of the results.\n",
        " - Is the problem solved in an appropriate way?\n",
        "- Are different hyperparameters and data augmentation used for training and are the results compared and analyzed?\n",
        "- Is the use of appropriate visualization tools and experiment tools for your reports?\n",
        "e.g., GradCAM, Weight & Biases, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VcoyrJVwzlS",
        "outputId": "936713e2-9471-4713-e9e5-32817661e447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting adabelief-pytorch\n",
            "  Downloading adabelief_pytorch-0.2.1-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from adabelief-pytorch) (2.1.0+cu118)\n",
            "Collecting colorama>=0.4.0 (from adabelief-pytorch)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: tabulate>=0.7 in /usr/local/lib/python3.10/dist-packages (from adabelief-pytorch) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->adabelief-pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.0->adabelief-pytorch) (1.3.0)\n",
            "Installing collected packages: colorama, adabelief-pytorch\n",
            "Successfully installed adabelief-pytorch-0.2.1 colorama-0.4.6\n"
          ]
        }
      ],
      "source": [
        "!pip install adabelief-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuomYJ5liLKA",
        "outputId": "e108461b-019d-4612-b27b-40b2cdb3c3b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import ToTensor,Normalize, RandomHorizontalFlip, Resize\n",
        "from torch.utils.data import ConcatDataset\n",
        "from torchvision.transforms import RandAugment\n",
        "\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from PIL import Image,ImageFilter,ImageEnhance\n",
        "\n",
        "from adabelief_pytorch import AdaBelief\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "from collections import Counter\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "\n",
        "# Set random seed\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(seed)\n",
        "\n",
        "### GPU Setting ###\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "print(DEVICE)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZjNYzcBPPgTb"
      },
      "outputs": [],
      "source": [
        "# !pip install wandb\n",
        "# import wandb\n",
        "# wandb.login()\n",
        "\n",
        "# # Initialize wandb\n",
        "# wandb.init(project=\"gpt5\")\n",
        "# config = wandb.config\n",
        "# config.dropout = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HamlO5MKZ-wk"
      },
      "outputs": [],
      "source": [
        "\n",
        "### Custom Dataset ###\n",
        "class CUB2011(Dataset):\n",
        "  def __init__(self, transform, mode='train'):\n",
        "    self.transform = transform\n",
        "    self.mode = mode\n",
        "\n",
        "    if self.mode == 'train':\n",
        "      self.image_folder = os.listdir('/content/drive/MyDrive/datasets/train')\n",
        "    elif self.mode == 'valid':\n",
        "      self.image_folder = os.listdir('/content/drive/MyDrive/datasets/valid')\n",
        "    elif self.mode == 'test':\n",
        "      self.image_folder = os.listdir('/content/drive/MyDrive/datasets/test')\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_folder)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = self.image_folder[idx]\n",
        "    img = Image.open(os.path.join('/content/drive/MyDrive/datasets', self.mode, img_path)).convert('RGB')\n",
        "    img = self.transform(img)\n",
        "\n",
        "\n",
        "    label = img_path.split('_')[-1].split('.')[0]\n",
        "    label = int(label)\n",
        "    return (img, label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "om7k9Cz5uytV"
      },
      "outputs": [],
      "source": [
        "# Geomentric transform + Visual corruptions\n",
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0, std=1, p=0.5):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if np.random.rand() < self.p:\n",
        "            img_array = np.array(img)\n",
        "            noise = np.random.normal(self.mean, self.std, img_array.shape)\n",
        "            noisy_image = np.clip(img_array + noise, 0, 255)  # Clip values to the range [0, 255]\n",
        "            return Image.fromarray(noisy_image.astype(np.uint8))\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std}, p={self.p})'\n",
        "\n",
        "\n",
        "class AdjustContrast(object):\n",
        "    def __init__(self, factor=1.0):\n",
        "        self.factor = factor\n",
        "\n",
        "    def __call__(self, img):\n",
        "        enhancer = ImageEnhance.Contrast(img)\n",
        "        img = enhancer.enhance(self.factor)\n",
        "        return img\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + f'(factor={self.factor})'\n",
        "\n",
        "class AdjustBrightness(object):\n",
        "    def __init__(self, factor=1.0):\n",
        "        self.factor = factor\n",
        "\n",
        "    def __call__(self, img):\n",
        "        enhancer = ImageEnhance.Brightness(img)\n",
        "        img = enhancer.enhance(self.factor)\n",
        "        return img\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + f'(factor={self.factor})'\n",
        "\n",
        "\n",
        "# Mix up\n",
        "def mixup_data(x, y, alpha=1.0):\n",
        "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size, device=x.device, dtype=torch.long)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c1qiR6PiqKL",
        "outputId": "d43daaa6-4b47-468a-fb1c-55091f1fe19d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of each dataset:  7080 296 298\n",
            "Num of each dataset:  2360 2360 2360 2360\n",
            "Loaded dataloader\n"
          ]
        }
      ],
      "source": [
        "### Data Preprocessing ###\n",
        "transforms_train_origin = transforms.Compose([transforms.Resize((448,448), Image.BICUBIC),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                                       ])\n",
        "transforms_test = transforms.Compose([transforms.Resize((448,448), Image.BICUBIC),\n",
        "                                       transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                                       ])\n",
        "\n",
        "# Apply RandAugment\n",
        "transforms_train_rand = transforms.Compose([\n",
        "    transforms.Resize((448, 448), Image.BICUBIC),\n",
        "    RandAugment(5,3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Geomentric transform + Visual corruptions\n",
        "transforms_train_g_v = transforms.Compose([\n",
        "    transforms.Resize((448, 448), Image.BICUBIC),\n",
        "    transforms.RandomResizedCrop(448),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.RandomVerticalFlip(0.5),\n",
        "    transforms.RandomRotation(30),\n",
        "    AddGaussianNoise(mean=0, std=25, p=0.5),\n",
        "    AdjustContrast(factor=2.0),\n",
        "    AdjustBrightness(factor=1.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_set_origin = CUB2011(mode='train',\n",
        "                    transform=transforms_train_origin)\n",
        "val_set = CUB2011(mode='valid',\n",
        "                  transform=transforms_test)\n",
        "test_set = CUB2011(mode='test',\n",
        "                  transform=transforms_test)\n",
        "\n",
        "# Duplicate original data for data augmentation\n",
        "train_set_augmented = CUB2011(mode='train', transform=transforms_train_rand)\n",
        "train_set_augmented2 = CUB2011(mode='train',transform=transforms_train_g_v)\n",
        "\n",
        "train_loader = DataLoader(train_set_origin,batch_size=BATCH_SIZE,shuffle=True)\n",
        "\n",
        "# Add mixed-up images to the train_loader\n",
        "train_set_mixup = []\n",
        "for input, target in train_loader:\n",
        "    mixed_input, target_a, target_b, lam = mixup_data(input, target, alpha=1.0)\n",
        "    train_set_mixup.append((mixed_input, target_a, target_b, lam))\n",
        "train_set_mixup = transforms.Compose([transforms.Resize((448,448), Image.BICUBIC),\n",
        "                                       transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                                       ])\n",
        "train_set_mixup = CUB2011(mode='train', transform=train_set_mixup)\n",
        "\n",
        "\n",
        "\n",
        "# Concatenate the datasets to create a new training dataset\n",
        "train_set_combined = ConcatDataset([train_set_origin,train_set_augmented2,train_set_mixup])\n",
        "\n",
        "print('Num of each dataset: ',len(train_set_combined),len(val_set),len(test_set))\n",
        "print('Num of each dataset: ',len(train_set_origin),len(train_set_augmented), len(train_set_augmented2), len(train_set_mixup))\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "# DataLoader class creates mini-batches for training deep learning models\n",
        "# DataLoader divides the entire dataset into batches of size BATCH_SIZE\n",
        "# train_loader = DataLoader(train_set_combined, batch_size=BATCH_SIZE, shuffle=True)\n",
        "train_loader = DataLoader(train_set_combined, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_set,batch_size=BATCH_SIZE,shuffle=False)\n",
        "test_loader = DataLoader(test_set,batch_size=BATCH_SIZE,shuffle=False)\n",
        "\n",
        "print(\"Loaded dataloader\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BCFpbfxxlNe",
        "outputId": "63114294-b910-4b16-c483-5332ae5d419e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: adabelief-pytorch in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from adabelief-pytorch) (2.1.0+cu118)\n",
            "Requirement already satisfied: colorama>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from adabelief-pytorch) (0.4.6)\n",
            "Requirement already satisfied: tabulate>=0.7 in /usr/local/lib/python3.10/dist-packages (from adabelief-pytorch) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->adabelief-pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.0->adabelief-pytorch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade adabelief-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1E8YNNbiwOC",
        "outputId": "5b282357-31f6-4555-b61b-f9e59bc0ee1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
            "\u001b[31mModifications to default arguments:\n",
            "\u001b[31m                           eps  weight_decouple    rectify\n",
            "-----------------------  -----  -----------------  ---------\n",
            "adabelief-pytorch=0.0.5  1e-08  False              False\n",
            ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
            "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
            "----------------------------------------------------------  ----------------------------------------------\n",
            "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
            "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
            "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
            "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
            "\u001b[0m\n",
            "Weight decoupling enabled in AdaBelief\n",
            "Rectification enabled in AdaBelief\n",
            "Created a learning model and optimizer\n"
          ]
        }
      ],
      "source": [
        "### Model / Optimizer ###\n",
        "EPOCH = 30\n",
        "\n",
        "lr = 0.0001\n",
        "\n",
        "model = models.densenet121(pretrained=True)\n",
        "\n",
        "# Freeze all layers of the pre-trained model\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the classifier (final fully connected layers)\n",
        "num_features = model.classifier.in_features\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(num_features, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(512, 50)  # Adjust based on the number of classes\n",
        ")\n",
        "model.to(DEVICE)\n",
        "\n",
        "# Apply Weight Decay: Prevent overfitting with L2 regularization\n",
        "optimizer = AdaBelief(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-16, weight_decouple=True, rectify=True)\n",
        "# optimizer = AdaBelief(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-16, weight_decouple=True, rectify=True, weight_decay=1e-5)\n",
        "\n",
        "# Initialize Cosine Annealing LR Scheduler\n",
        "\n",
        "dataset_size = len(train_set_combined) # Size of the training dataset\n",
        "T_max = dataset_size / BATCH_SIZE  # Number of epochs for one cycle\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=T_max, eta_min=0.001)\n",
        "\n",
        "print(\"Created a learning model and optimizer\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mTQsnbhbzm7C"
      },
      "outputs": [],
      "source": [
        "### Train/Evaluation ###\n",
        "def train(model,train_loader,optimizer,epoch):\n",
        "  model.train()\n",
        "  for i,(image,target) in enumerate(train_loader):\n",
        "    image,target = image.to(DEVICE),target.to(DEVICE)\n",
        "    output = model(image)\n",
        "    optimizer.zero_grad()\n",
        "    train_loss = F.cross_entropy(output,target).to(DEVICE)\n",
        "\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Implement gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.25)  # You can adjust the 'max_norm' parameter as needed\n",
        "\n",
        "    # Update the scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    if i%10 ==0:\n",
        "      print(\n",
        "          f'Train Epoch: {epoch} [{i}/{len(train_loader)}]\\tloss: {train_loss.item():6f}')\n",
        "\n",
        "  return train_loss\n",
        "\n",
        "def evaluate(model,val_loader):\n",
        "  model.eval()\n",
        "  eval_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for i,(image,target) in enumerate(val_loader):\n",
        "      image,target = image.to(DEVICE),target.to(DEVICE)\n",
        "      output = model(image)\n",
        "\n",
        "      eval_loss += F.cross_entropy(output,target, reduction='sum').item()\n",
        "      pred = output.max(1,keepdim=True)[1]\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  eval_loss /= len(val_loader.dataset)\n",
        "  eval_accuracy = 100*correct / len(val_loader.dataset)\n",
        "  return eval_loss,eval_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ2smo7ucLY0",
        "outputId": "ddb0ceed-9637-4077-899d-a16e4b754478"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/111]\tloss: 3.893871\n",
            "Train Epoch: 0 [10/111]\tloss: 3.919055\n",
            "Train Epoch: 0 [20/111]\tloss: 3.937292\n",
            "Train Epoch: 0 [30/111]\tloss: 3.962585\n",
            "Train Epoch: 0 [40/111]\tloss: 3.916118\n",
            "Train Epoch: 0 [50/111]\tloss: 3.872267\n",
            "Train Epoch: 0 [60/111]\tloss: 3.849238\n",
            "Train Epoch: 0 [70/111]\tloss: 3.831191\n",
            "Train Epoch: 0 [80/111]\tloss: 3.840073\n",
            "Train Epoch: 0 [90/111]\tloss: 3.803045\n",
            "Train Epoch: 0 [100/111]\tloss: 3.797223\n",
            "Train Epoch: 0 [110/111]\tloss: 3.732400\n",
            "[0]Validation Loss: 3.6746,Accuracy: 15.8784%\n",
            "Train Epoch: 1 [0/111]\tloss: 3.679848\n",
            "Train Epoch: 1 [10/111]\tloss: 3.724938\n",
            "Train Epoch: 1 [20/111]\tloss: 3.716075\n",
            "Train Epoch: 1 [30/111]\tloss: 3.627262\n",
            "Train Epoch: 1 [40/111]\tloss: 3.573429\n",
            "Train Epoch: 1 [50/111]\tloss: 3.486635\n",
            "Train Epoch: 1 [60/111]\tloss: 3.494348\n",
            "Train Epoch: 1 [70/111]\tloss: 3.357762\n",
            "Train Epoch: 1 [80/111]\tloss: 3.362581\n",
            "Train Epoch: 1 [90/111]\tloss: 3.379961\n",
            "Train Epoch: 1 [100/111]\tloss: 3.448116\n",
            "Train Epoch: 1 [110/111]\tloss: 3.403067\n",
            "[1]Validation Loss: 3.1964,Accuracy: 36.1486%\n",
            "Train Epoch: 2 [0/111]\tloss: 3.263239\n",
            "Train Epoch: 2 [10/111]\tloss: 3.175570\n",
            "Train Epoch: 2 [20/111]\tloss: 3.356858\n",
            "Train Epoch: 2 [30/111]\tloss: 3.223584\n",
            "Train Epoch: 2 [40/111]\tloss: 3.249085\n",
            "Train Epoch: 2 [50/111]\tloss: 3.203775\n",
            "Train Epoch: 2 [60/111]\tloss: 3.268952\n",
            "Train Epoch: 2 [70/111]\tloss: 3.198782\n",
            "Train Epoch: 2 [80/111]\tloss: 3.245495\n",
            "Train Epoch: 2 [90/111]\tloss: 3.154179\n",
            "Train Epoch: 2 [100/111]\tloss: 2.939154\n",
            "Train Epoch: 2 [110/111]\tloss: 3.040032\n",
            "[2]Validation Loss: 2.7755,Accuracy: 32.0946%\n",
            "Train Epoch: 3 [0/111]\tloss: 3.093011\n",
            "Train Epoch: 3 [10/111]\tloss: 2.779296\n",
            "Train Epoch: 3 [20/111]\tloss: 2.983771\n",
            "Train Epoch: 3 [30/111]\tloss: 2.949212\n",
            "Train Epoch: 3 [40/111]\tloss: 2.668106\n",
            "Train Epoch: 3 [50/111]\tloss: 2.697916\n",
            "Train Epoch: 3 [60/111]\tloss: 2.583595\n",
            "Train Epoch: 3 [70/111]\tloss: 2.598083\n",
            "Train Epoch: 3 [80/111]\tloss: 2.532168\n",
            "Train Epoch: 3 [90/111]\tloss: 2.611297\n",
            "Train Epoch: 3 [100/111]\tloss: 2.674341\n",
            "Train Epoch: 3 [110/111]\tloss: 2.540044\n",
            "[3]Validation Loss: 2.2911,Accuracy: 56.0811%\n",
            "Train Epoch: 4 [0/111]\tloss: 2.569435\n",
            "Train Epoch: 4 [10/111]\tloss: 2.429271\n",
            "Train Epoch: 4 [20/111]\tloss: 2.511403\n",
            "Train Epoch: 4 [30/111]\tloss: 2.603221\n",
            "Train Epoch: 4 [40/111]\tloss: 2.302849\n",
            "Train Epoch: 4 [50/111]\tloss: 2.448976\n",
            "Train Epoch: 4 [60/111]\tloss: 2.566019\n",
            "Train Epoch: 4 [70/111]\tloss: 2.496060\n",
            "Train Epoch: 4 [80/111]\tloss: 2.476560\n",
            "Train Epoch: 4 [90/111]\tloss: 2.269675\n",
            "Train Epoch: 4 [100/111]\tloss: 2.081538\n",
            "Train Epoch: 4 [110/111]\tloss: 2.112526\n",
            "[4]Validation Loss: 1.9849,Accuracy: 56.4189%\n",
            "Train Epoch: 5 [0/111]\tloss: 2.386545\n",
            "Train Epoch: 5 [10/111]\tloss: 2.446550\n",
            "Train Epoch: 5 [20/111]\tloss: 2.128397\n",
            "Train Epoch: 5 [30/111]\tloss: 2.177307\n",
            "Train Epoch: 5 [40/111]\tloss: 2.046285\n",
            "Train Epoch: 5 [50/111]\tloss: 1.844615\n",
            "Train Epoch: 5 [60/111]\tloss: 1.855413\n",
            "Train Epoch: 5 [70/111]\tloss: 1.689506\n",
            "Train Epoch: 5 [80/111]\tloss: 1.936613\n",
            "Train Epoch: 5 [90/111]\tloss: 2.070202\n",
            "Train Epoch: 5 [100/111]\tloss: 1.995575\n",
            "Train Epoch: 5 [110/111]\tloss: 1.782943\n",
            "[5]Validation Loss: 1.6244,Accuracy: 68.2432%\n",
            "Train Epoch: 6 [0/111]\tloss: 1.781657\n",
            "Train Epoch: 6 [10/111]\tloss: 1.873203\n",
            "Train Epoch: 6 [20/111]\tloss: 1.650538\n",
            "Train Epoch: 6 [30/111]\tloss: 1.911594\n",
            "Train Epoch: 6 [40/111]\tloss: 1.917354\n",
            "Train Epoch: 6 [50/111]\tloss: 1.825285\n",
            "Train Epoch: 6 [60/111]\tloss: 1.878656\n",
            "Train Epoch: 6 [70/111]\tloss: 2.093125\n",
            "Train Epoch: 6 [80/111]\tloss: 1.814859\n",
            "Train Epoch: 6 [90/111]\tloss: 1.994150\n",
            "Train Epoch: 6 [100/111]\tloss: 1.897506\n",
            "Train Epoch: 6 [110/111]\tloss: 1.897591\n",
            "[6]Validation Loss: 1.4968,Accuracy: 65.8784%\n",
            "Train Epoch: 7 [0/111]\tloss: 2.041092\n",
            "Train Epoch: 7 [10/111]\tloss: 1.881349\n",
            "Train Epoch: 7 [20/111]\tloss: 1.750473\n",
            "Train Epoch: 7 [30/111]\tloss: 1.480245\n",
            "Train Epoch: 7 [40/111]\tloss: 1.530426\n",
            "Train Epoch: 7 [50/111]\tloss: 1.653008\n",
            "Train Epoch: 7 [60/111]\tloss: 1.651532\n",
            "Train Epoch: 7 [70/111]\tloss: 1.549439\n",
            "Train Epoch: 7 [80/111]\tloss: 1.230877\n",
            "Train Epoch: 7 [90/111]\tloss: 1.634363\n",
            "Train Epoch: 7 [100/111]\tloss: 1.438010\n",
            "Train Epoch: 7 [110/111]\tloss: 1.496736\n",
            "[7]Validation Loss: 1.2447,Accuracy: 71.2838%\n",
            "Train Epoch: 8 [0/111]\tloss: 1.554655\n",
            "Train Epoch: 8 [10/111]\tloss: 1.248304\n",
            "Train Epoch: 8 [20/111]\tloss: 1.607788\n",
            "Train Epoch: 8 [30/111]\tloss: 1.202672\n",
            "Train Epoch: 8 [40/111]\tloss: 1.690466\n",
            "Train Epoch: 8 [50/111]\tloss: 1.537518\n",
            "Train Epoch: 8 [60/111]\tloss: 1.373840\n",
            "Train Epoch: 8 [70/111]\tloss: 1.545828\n",
            "Train Epoch: 8 [80/111]\tloss: 1.542314\n",
            "Train Epoch: 8 [90/111]\tloss: 1.529230\n",
            "Train Epoch: 8 [100/111]\tloss: 1.781564\n",
            "Train Epoch: 8 [110/111]\tloss: 1.560776\n",
            "[8]Validation Loss: 1.2250,Accuracy: 70.6081%\n",
            "Train Epoch: 9 [0/111]\tloss: 1.808411\n",
            "Train Epoch: 9 [10/111]\tloss: 1.362551\n",
            "Train Epoch: 9 [20/111]\tloss: 1.621632\n",
            "Train Epoch: 9 [30/111]\tloss: 1.267330\n",
            "Train Epoch: 9 [40/111]\tloss: 1.520309\n",
            "Train Epoch: 9 [50/111]\tloss: 1.767606\n",
            "Train Epoch: 9 [60/111]\tloss: 1.355873\n",
            "Train Epoch: 9 [70/111]\tloss: 1.068328\n",
            "Train Epoch: 9 [80/111]\tloss: 1.237041\n",
            "Train Epoch: 9 [90/111]\tloss: 1.565363\n",
            "Train Epoch: 9 [100/111]\tloss: 1.596448\n",
            "Train Epoch: 9 [110/111]\tloss: 1.883699\n",
            "[9]Validation Loss: 1.0393,Accuracy: 72.6351%\n",
            "Train Epoch: 10 [0/111]\tloss: 1.276426\n",
            "Train Epoch: 10 [10/111]\tloss: 1.360071\n",
            "Train Epoch: 10 [20/111]\tloss: 1.420854\n",
            "Train Epoch: 10 [30/111]\tloss: 1.107686\n",
            "Train Epoch: 10 [40/111]\tloss: 1.261825\n",
            "Train Epoch: 10 [50/111]\tloss: 1.195282\n",
            "Train Epoch: 10 [60/111]\tloss: 1.451280\n",
            "Train Epoch: 10 [70/111]\tloss: 1.631963\n",
            "Train Epoch: 10 [80/111]\tloss: 1.496549\n",
            "Train Epoch: 10 [90/111]\tloss: 1.327943\n",
            "Train Epoch: 10 [100/111]\tloss: 1.380952\n",
            "Train Epoch: 10 [110/111]\tloss: 1.493341\n",
            "[10]Validation Loss: 1.0624,Accuracy: 70.6081%\n",
            "Train Epoch: 11 [0/111]\tloss: 1.588063\n",
            "Train Epoch: 11 [10/111]\tloss: 1.368874\n",
            "Train Epoch: 11 [20/111]\tloss: 1.387278\n",
            "Train Epoch: 11 [30/111]\tloss: 1.223428\n",
            "Train Epoch: 11 [40/111]\tloss: 1.331093\n",
            "Train Epoch: 11 [50/111]\tloss: 1.409336\n",
            "Train Epoch: 11 [60/111]\tloss: 1.331262\n",
            "Train Epoch: 11 [70/111]\tloss: 1.153700\n",
            "Train Epoch: 11 [80/111]\tloss: 0.807636\n",
            "Train Epoch: 11 [90/111]\tloss: 1.282907\n",
            "Train Epoch: 11 [100/111]\tloss: 1.654742\n",
            "Train Epoch: 11 [110/111]\tloss: 1.612347\n",
            "[11]Validation Loss: 0.9139,Accuracy: 74.6622%\n",
            "Train Epoch: 12 [0/111]\tloss: 1.181972\n",
            "Train Epoch: 12 [10/111]\tloss: 0.797501\n",
            "Train Epoch: 12 [20/111]\tloss: 1.125244\n",
            "Train Epoch: 12 [30/111]\tloss: 1.254557\n",
            "Train Epoch: 12 [40/111]\tloss: 1.087073\n",
            "Train Epoch: 12 [50/111]\tloss: 0.939337\n",
            "Train Epoch: 12 [60/111]\tloss: 1.647562\n",
            "Train Epoch: 12 [70/111]\tloss: 1.193348\n",
            "Train Epoch: 12 [80/111]\tloss: 1.442413\n",
            "Train Epoch: 12 [90/111]\tloss: 1.244169\n",
            "Train Epoch: 12 [100/111]\tloss: 1.383951\n",
            "Train Epoch: 12 [110/111]\tloss: 0.937484\n",
            "[12]Validation Loss: 0.9681,Accuracy: 75.0000%\n",
            "Train Epoch: 13 [0/111]\tloss: 1.151647\n",
            "Train Epoch: 13 [10/111]\tloss: 1.355160\n",
            "Train Epoch: 13 [20/111]\tloss: 1.242212\n",
            "Train Epoch: 13 [30/111]\tloss: 1.281366\n",
            "Train Epoch: 13 [40/111]\tloss: 1.219995\n",
            "Train Epoch: 13 [50/111]\tloss: 1.500822\n",
            "Train Epoch: 13 [60/111]\tloss: 1.336333\n",
            "Train Epoch: 13 [70/111]\tloss: 1.111417\n",
            "Train Epoch: 13 [80/111]\tloss: 1.456569\n",
            "Train Epoch: 13 [90/111]\tloss: 1.233712\n",
            "Train Epoch: 13 [100/111]\tloss: 1.240400\n",
            "Train Epoch: 13 [110/111]\tloss: 1.568790\n",
            "[13]Validation Loss: 0.8168,Accuracy: 78.0405%\n",
            "Train Epoch: 14 [0/111]\tloss: 1.127992\n",
            "Train Epoch: 14 [10/111]\tloss: 1.335743\n",
            "Train Epoch: 14 [20/111]\tloss: 1.004377\n",
            "Train Epoch: 14 [30/111]\tloss: 1.287288\n",
            "Train Epoch: 14 [40/111]\tloss: 1.247750\n",
            "Train Epoch: 14 [50/111]\tloss: 1.122677\n",
            "Train Epoch: 14 [60/111]\tloss: 1.137975\n",
            "Train Epoch: 14 [70/111]\tloss: 1.123231\n",
            "Train Epoch: 14 [80/111]\tloss: 1.383034\n",
            "Train Epoch: 14 [90/111]\tloss: 1.233949\n",
            "Train Epoch: 14 [100/111]\tloss: 1.039615\n",
            "Train Epoch: 14 [110/111]\tloss: 1.375499\n",
            "[14]Validation Loss: 0.8924,Accuracy: 75.0000%\n",
            "Train Epoch: 15 [0/111]\tloss: 1.079008\n",
            "Train Epoch: 15 [10/111]\tloss: 1.103491\n",
            "Train Epoch: 15 [20/111]\tloss: 1.254979\n",
            "Train Epoch: 15 [30/111]\tloss: 1.128927\n",
            "Train Epoch: 15 [40/111]\tloss: 1.250234\n",
            "Train Epoch: 15 [50/111]\tloss: 1.370796\n",
            "Train Epoch: 15 [60/111]\tloss: 0.968234\n",
            "Train Epoch: 15 [70/111]\tloss: 1.462233\n",
            "Train Epoch: 15 [80/111]\tloss: 0.722564\n",
            "Train Epoch: 15 [90/111]\tloss: 1.018638\n",
            "Train Epoch: 15 [100/111]\tloss: 1.174265\n",
            "Train Epoch: 15 [110/111]\tloss: 1.000860\n",
            "[15]Validation Loss: 0.7703,Accuracy: 77.0270%\n",
            "Train Epoch: 16 [0/111]\tloss: 1.013160\n",
            "Train Epoch: 16 [10/111]\tloss: 1.093519\n",
            "Train Epoch: 16 [20/111]\tloss: 0.824420\n",
            "Train Epoch: 16 [30/111]\tloss: 1.099471\n",
            "Train Epoch: 16 [40/111]\tloss: 1.374975\n",
            "Train Epoch: 16 [50/111]\tloss: 1.197447\n",
            "Train Epoch: 16 [60/111]\tloss: 1.087086\n",
            "Train Epoch: 16 [70/111]\tloss: 0.920366\n",
            "Train Epoch: 16 [80/111]\tloss: 0.870273\n",
            "Train Epoch: 16 [90/111]\tloss: 1.420379\n",
            "Train Epoch: 16 [100/111]\tloss: 1.020426\n",
            "Train Epoch: 16 [110/111]\tloss: 1.501013\n",
            "[16]Validation Loss: 0.8359,Accuracy: 76.6892%\n",
            "Train Epoch: 17 [0/111]\tloss: 1.546314\n",
            "Train Epoch: 17 [10/111]\tloss: 1.121583\n",
            "Train Epoch: 17 [20/111]\tloss: 1.075225\n",
            "Train Epoch: 17 [30/111]\tloss: 1.349128\n",
            "Train Epoch: 17 [40/111]\tloss: 1.038227\n",
            "Train Epoch: 17 [50/111]\tloss: 1.135122\n",
            "Train Epoch: 17 [60/111]\tloss: 1.289012\n",
            "Train Epoch: 17 [70/111]\tloss: 1.213294\n",
            "Train Epoch: 17 [80/111]\tloss: 1.081954\n",
            "Train Epoch: 17 [90/111]\tloss: 0.806133\n",
            "Train Epoch: 17 [100/111]\tloss: 1.205597\n",
            "Train Epoch: 17 [110/111]\tloss: 1.023831\n",
            "[17]Validation Loss: 0.7416,Accuracy: 76.6892%\n",
            "Train Epoch: 18 [0/111]\tloss: 1.174620\n",
            "Train Epoch: 18 [10/111]\tloss: 0.992831\n",
            "Train Epoch: 18 [20/111]\tloss: 0.761612\n",
            "Train Epoch: 18 [30/111]\tloss: 0.770380\n",
            "Train Epoch: 18 [40/111]\tloss: 1.173320\n",
            "Train Epoch: 18 [50/111]\tloss: 1.037616\n",
            "Train Epoch: 18 [60/111]\tloss: 1.174702\n",
            "Train Epoch: 18 [70/111]\tloss: 1.055438\n",
            "Train Epoch: 18 [80/111]\tloss: 1.215701\n",
            "Train Epoch: 18 [90/111]\tloss: 0.970505\n",
            "Train Epoch: 18 [100/111]\tloss: 0.972979\n",
            "Train Epoch: 18 [110/111]\tloss: 0.942220\n",
            "[18]Validation Loss: 0.7676,Accuracy: 77.3649%\n",
            "Train Epoch: 19 [0/111]\tloss: 1.072487\n",
            "Train Epoch: 19 [10/111]\tloss: 1.424109\n",
            "Train Epoch: 19 [20/111]\tloss: 1.146800\n",
            "Train Epoch: 19 [30/111]\tloss: 0.925318\n",
            "Train Epoch: 19 [40/111]\tloss: 1.044518\n",
            "Train Epoch: 19 [50/111]\tloss: 0.997885\n",
            "Train Epoch: 19 [60/111]\tloss: 0.983361\n",
            "Train Epoch: 19 [70/111]\tloss: 1.001017\n",
            "Train Epoch: 19 [80/111]\tloss: 1.255725\n",
            "Train Epoch: 19 [90/111]\tloss: 0.926481\n",
            "Train Epoch: 19 [100/111]\tloss: 1.093265\n",
            "Train Epoch: 19 [110/111]\tloss: 0.774528\n",
            "[19]Validation Loss: 0.7165,Accuracy: 77.7027%\n",
            "Train Epoch: 20 [0/111]\tloss: 0.778629\n",
            "Train Epoch: 20 [10/111]\tloss: 0.820806\n",
            "Train Epoch: 20 [20/111]\tloss: 0.966680\n",
            "Train Epoch: 20 [30/111]\tloss: 1.097250\n",
            "Train Epoch: 20 [40/111]\tloss: 1.380622\n",
            "Train Epoch: 20 [50/111]\tloss: 0.865419\n",
            "Train Epoch: 20 [60/111]\tloss: 1.144628\n",
            "Train Epoch: 20 [70/111]\tloss: 1.017140\n",
            "Train Epoch: 20 [80/111]\tloss: 1.232694\n",
            "Train Epoch: 20 [90/111]\tloss: 0.964265\n",
            "Train Epoch: 20 [100/111]\tloss: 1.059492\n",
            "Train Epoch: 20 [110/111]\tloss: 1.022135\n",
            "[20]Validation Loss: 0.7935,Accuracy: 77.0270%\n",
            "Train Epoch: 21 [0/111]\tloss: 1.167078\n",
            "Train Epoch: 21 [10/111]\tloss: 1.459011\n",
            "Train Epoch: 21 [20/111]\tloss: 1.156355\n",
            "Train Epoch: 21 [30/111]\tloss: 1.120214\n",
            "Train Epoch: 21 [40/111]\tloss: 1.048788\n",
            "Train Epoch: 21 [50/111]\tloss: 0.899928\n",
            "Train Epoch: 21 [60/111]\tloss: 0.798610\n",
            "Train Epoch: 21 [70/111]\tloss: 1.196664\n",
            "Train Epoch: 21 [80/111]\tloss: 1.059091\n",
            "Train Epoch: 21 [90/111]\tloss: 1.083793\n",
            "Train Epoch: 21 [100/111]\tloss: 0.850365\n",
            "Train Epoch: 21 [110/111]\tloss: 0.770034\n",
            "[21]Validation Loss: 0.7016,Accuracy: 78.0405%\n",
            "Train Epoch: 22 [0/111]\tloss: 0.874440\n",
            "Train Epoch: 22 [10/111]\tloss: 0.696260\n",
            "Train Epoch: 22 [20/111]\tloss: 0.759729\n",
            "Train Epoch: 22 [30/111]\tloss: 1.168606\n",
            "Train Epoch: 22 [40/111]\tloss: 0.902110\n",
            "Train Epoch: 22 [50/111]\tloss: 0.796458\n",
            "Train Epoch: 22 [60/111]\tloss: 1.015072\n",
            "Train Epoch: 22 [70/111]\tloss: 1.106418\n",
            "Train Epoch: 22 [80/111]\tloss: 0.878237\n",
            "Train Epoch: 22 [90/111]\tloss: 1.428941\n",
            "Train Epoch: 22 [100/111]\tloss: 0.948805\n",
            "Train Epoch: 22 [110/111]\tloss: 1.037984\n",
            "[22]Validation Loss: 0.7543,Accuracy: 77.7027%\n",
            "Train Epoch: 23 [0/111]\tloss: 1.088444\n",
            "Train Epoch: 23 [10/111]\tloss: 1.330988\n",
            "Train Epoch: 23 [20/111]\tloss: 1.070730\n",
            "Train Epoch: 23 [30/111]\tloss: 1.328998\n",
            "Train Epoch: 23 [40/111]\tloss: 1.131412\n",
            "Train Epoch: 23 [50/111]\tloss: 1.435677\n",
            "Train Epoch: 23 [60/111]\tloss: 0.872435\n",
            "Train Epoch: 23 [70/111]\tloss: 0.859220\n",
            "Train Epoch: 23 [80/111]\tloss: 0.932623\n",
            "Train Epoch: 23 [90/111]\tloss: 0.740465\n",
            "Train Epoch: 23 [100/111]\tloss: 0.734093\n",
            "Train Epoch: 23 [110/111]\tloss: 0.889618\n",
            "[23]Validation Loss: 0.6815,Accuracy: 79.0541%\n",
            "Train Epoch: 24 [0/111]\tloss: 0.961639\n",
            "Train Epoch: 24 [10/111]\tloss: 0.809064\n",
            "Train Epoch: 24 [20/111]\tloss: 0.640736\n",
            "Train Epoch: 24 [30/111]\tloss: 0.667550\n",
            "Train Epoch: 24 [40/111]\tloss: 0.748996\n",
            "Train Epoch: 24 [50/111]\tloss: 0.959902\n",
            "Train Epoch: 24 [60/111]\tloss: 0.725798\n",
            "Train Epoch: 24 [70/111]\tloss: 1.014015\n",
            "Train Epoch: 24 [80/111]\tloss: 0.780916\n",
            "Train Epoch: 24 [90/111]\tloss: 1.147509\n",
            "Train Epoch: 24 [100/111]\tloss: 1.097734\n",
            "Train Epoch: 24 [110/111]\tloss: 1.092316\n",
            "[24]Validation Loss: 0.7751,Accuracy: 75.3378%\n",
            "Train Epoch: 25 [0/111]\tloss: 0.904999\n",
            "Train Epoch: 25 [10/111]\tloss: 1.182992\n",
            "Train Epoch: 25 [20/111]\tloss: 1.344649\n",
            "Train Epoch: 25 [30/111]\tloss: 0.836266\n",
            "Train Epoch: 25 [40/111]\tloss: 0.852628\n",
            "Train Epoch: 25 [50/111]\tloss: 1.067409\n",
            "Train Epoch: 25 [60/111]\tloss: 0.955387\n",
            "Train Epoch: 25 [70/111]\tloss: 1.035643\n",
            "Train Epoch: 25 [80/111]\tloss: 0.748800\n",
            "Train Epoch: 25 [90/111]\tloss: 0.858410\n",
            "Train Epoch: 25 [100/111]\tloss: 0.679410\n",
            "Train Epoch: 25 [110/111]\tloss: 1.135074\n",
            "[25]Validation Loss: 0.6903,Accuracy: 78.0405%\n",
            "Train Epoch: 26 [0/111]\tloss: 0.917927\n",
            "Train Epoch: 26 [10/111]\tloss: 0.607787\n",
            "Train Epoch: 26 [20/111]\tloss: 0.735290\n",
            "Train Epoch: 26 [30/111]\tloss: 0.816097\n",
            "Train Epoch: 26 [40/111]\tloss: 0.792112\n",
            "Train Epoch: 26 [50/111]\tloss: 0.813335\n",
            "Train Epoch: 26 [60/111]\tloss: 1.380299\n",
            "Train Epoch: 26 [70/111]\tloss: 0.686667\n",
            "Train Epoch: 26 [80/111]\tloss: 0.936872\n",
            "Train Epoch: 26 [90/111]\tloss: 1.099380\n",
            "Train Epoch: 26 [100/111]\tloss: 1.266098\n",
            "Train Epoch: 26 [110/111]\tloss: 0.796178\n",
            "[26]Validation Loss: 0.7619,Accuracy: 77.3649%\n",
            "Train Epoch: 27 [0/111]\tloss: 0.760749\n",
            "Train Epoch: 27 [10/111]\tloss: 0.802152\n",
            "Train Epoch: 27 [20/111]\tloss: 1.148741\n",
            "Train Epoch: 27 [30/111]\tloss: 1.031186\n",
            "Train Epoch: 27 [40/111]\tloss: 0.773144\n",
            "Train Epoch: 27 [50/111]\tloss: 1.021573\n",
            "Train Epoch: 27 [60/111]\tloss: 0.807923\n",
            "Train Epoch: 27 [70/111]\tloss: 0.817069\n",
            "Train Epoch: 27 [80/111]\tloss: 1.115896\n",
            "Train Epoch: 27 [90/111]\tloss: 0.812974\n",
            "Train Epoch: 27 [100/111]\tloss: 0.695601\n",
            "Train Epoch: 27 [110/111]\tloss: 0.772867\n",
            "[27]Validation Loss: 0.6728,Accuracy: 79.3919%\n",
            "Train Epoch: 28 [0/111]\tloss: 0.741029\n",
            "Train Epoch: 28 [10/111]\tloss: 0.978662\n",
            "Train Epoch: 28 [20/111]\tloss: 0.669224\n",
            "Train Epoch: 28 [30/111]\tloss: 1.020948\n",
            "Train Epoch: 28 [40/111]\tloss: 1.023321\n",
            "Train Epoch: 28 [50/111]\tloss: 0.736384\n",
            "Train Epoch: 28 [60/111]\tloss: 0.833055\n",
            "Train Epoch: 28 [70/111]\tloss: 1.287035\n",
            "Train Epoch: 28 [80/111]\tloss: 1.020424\n",
            "Train Epoch: 28 [90/111]\tloss: 0.867493\n",
            "Train Epoch: 28 [100/111]\tloss: 1.178960\n",
            "Train Epoch: 28 [110/111]\tloss: 0.873471\n",
            "[28]Validation Loss: 0.7346,Accuracy: 76.6892%\n",
            "Train Epoch: 29 [0/111]\tloss: 1.246235\n",
            "Train Epoch: 29 [10/111]\tloss: 1.574932\n",
            "Train Epoch: 29 [20/111]\tloss: 1.162092\n",
            "Train Epoch: 29 [30/111]\tloss: 1.167982\n",
            "Train Epoch: 29 [40/111]\tloss: 0.968745\n",
            "Train Epoch: 29 [50/111]\tloss: 0.891994\n",
            "Train Epoch: 29 [60/111]\tloss: 0.985991\n",
            "Train Epoch: 29 [70/111]\tloss: 0.986527\n",
            "Train Epoch: 29 [80/111]\tloss: 0.911326\n",
            "Train Epoch: 29 [90/111]\tloss: 0.610401\n",
            "Train Epoch: 29 [100/111]\tloss: 0.693357\n",
            "Train Epoch: 29 [110/111]\tloss: 0.814647\n",
            "[29]Validation Loss: 0.6748,Accuracy: 78.7162%\n",
            "[FINAL] Test Loss: 0.7193,Accuracy: 80.5369%\n",
            "Best Accuracy:  79.39189189189189\n",
            "Elasped Time: 1h, 83m, 1s\n",
            "time: 1h, 83m, 1s\n"
          ]
        }
      ],
      "source": [
        "#### Mian ###\n",
        "start = time.time()\n",
        "best = 0\n",
        "\n",
        "train_losses = []  # 훈련 손실을 저장할 목록\n",
        "val_losses = []    # 검증 손실을 저장할 목록\n",
        "val_accuracys = []\n",
        "\n",
        "# 디렉토리 생성\n",
        "os.makedirs('./best_model', exist_ok=True)\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "  train_loss = train(model,train_loader,optimizer,epoch)\n",
        "  val_loss,val_accuracy = evaluate(model,val_loader)\n",
        "\n",
        "  # # Log metrics to wandb\n",
        "  # wandb.log({\"loss\": loss.item()})\n",
        "\n",
        "  # 훈련 및 검증 손실을 목록에 추가\n",
        "  train_losses.append(train_loss)\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "  # Save best model\n",
        "  if val_accuracy > best:\n",
        "    best = val_accuracy\n",
        "    torch.save(model.state_dict(),\"./best_model.pth\")\n",
        "\n",
        "  val_accuracys.append(val_accuracy)\n",
        "  print(f\"[{epoch}]Validation Loss: {val_loss:.4f},Accuracy: {val_accuracy:.4f}%\")\n",
        "\n",
        "# Test result\n",
        "test_loss,test_accuracy = evaluate(model,test_loader)\n",
        "print(f'[FINAL] Test Loss: {test_loss:.4f},Accuracy: {test_accuracy:.4f}%')\n",
        "\n",
        "end = time.time()\n",
        "elasped_time = end - start\n",
        "\n",
        "print(\"Best Accuracy: \",best)\n",
        "print(\n",
        "    f\"Elasped Time: {int(elasped_time/3600)}h, {int(elasped_time/60)}m, {int(elasped_time%60)}s\")\n",
        "print(\n",
        "    f\"time: {int(elasped_time/3600)}h, {int(elasped_time/60)}m, {int(elasped_time%60)}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AtJ_lNacFqm"
      },
      "outputs": [],
      "source": [
        "# Plot the loss values\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Move train_loss and val_loss tensors to CPU and convert them to NumPy arrays\n",
        "train_losses = [loss.cpu().detach().numpy() if isinstance(loss, torch.Tensor) else loss for loss in train_losses]\n",
        "val_losses = [loss.\n",
        "              cpu().detach().numpy() if isinstance(loss, torch.Tensor) else loss for loss in val_losses]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, EPOCH + 1), train_losses, label='train_loss', marker='o')\n",
        "plt.plot(range(1, EPOCH + 1), val_losses, label='val_loss', marker='o')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "\n",
        "plt.xlabel('EPOCH')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "val_accuracys = [acc.cpu().detach().numpy() if isinstance(acc, torch.Tensor) else acc for acc in val_accuracys]\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, EPOCH + 1), val_accuracys, label='val_accuracy', marker='o')\n",
        "plt.title('Validation Accuracy Over Epochs')\n",
        "\n",
        "plt.xlabel('EPOCH')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbSDhRMYYvVy"
      },
      "outputs": [],
      "source": [
        "# !pip install git+https://github.com/jacobgil/pytorch-grad-cam.git\n",
        "\n",
        "# import copy\n",
        "# from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
        "# from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "# from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "# import numpy as np\n",
        "# from PIL import Image\n",
        "\n",
        "# import torchvision\n",
        "# #\n",
        "# # Pick up layers for visualization\n",
        "# target_layers = [model.layer4[-1]]\n",
        "# grad_cam = GradCam(model = model, target_layers, use_cuda=False)\n",
        "\n",
        "# # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
        "# grayscale_cam = cam(input_tensor=input_tensor)\n",
        "\n",
        "# # In this example grayscale_cam has only one image in the batch:\n",
        "# grayscale_cam = grayscale_cam[0, :]\n",
        "# visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDscRu5Ndahh"
      },
      "outputs": [],
      "source": [
        "# # gradCAM\n",
        "# import cv2\n",
        "# from google.colab.patches import cv2_imshow                                      # cv2.imshow() should be replaced to cv2_imshow() in google.colab\n",
        "# import glob\n",
        "# import numpy as np\n",
        "\n",
        "\n",
        "# class GradCam(nn.Module):\n",
        "#     def __init__(self, model, module, layer):\n",
        "#         super().__init__()\n",
        "#         self.model = model\n",
        "#         self.module = module\n",
        "#         self.layer = layer\n",
        "#         self.register_hooks()\n",
        "\n",
        "#     def register_hooks(self):\n",
        "#         for modue_name, module in self.model._modules.items():\n",
        "#             if modue_name == self.module:\n",
        "#                 for layer_name, module in module._modules.items():\n",
        "#                     if layer_name == self.layer:\n",
        "#                         module.register_forward_hook(self.forward_hook)\n",
        "#                         module.register_backward_hook(self.backward_hook)\n",
        "\n",
        "#     def forward(self, input, target_index):\n",
        "#         outs = self.model(input)\n",
        "#         outs = outs.squeeze()                                                    # [1, num_classes]  --> [num_classes]\n",
        "#         outs = outs.to(DEVICE)\n",
        "\n",
        "#         # 가장 큰 값을 가지는 것을 target index 로 사용\n",
        "#         if target_index is None:\n",
        "#             target_index = outs.argmax()\n",
        "#             target_index = target_index.to(DEVICE)\n",
        "\n",
        "#         outs[target_index].backward(retain_graph=True)\n",
        "#         a_k = torch.mean(self.backward_result, dim=(1, 2), keepdim=True)         # [512, 1, 1]\n",
        "#         out = torch.sum(a_k * self.forward_result, dim=0).cuda()                 # [512, 7, 7] * [512, 1, 1]\n",
        "#         out = torch.relu(out) / torch.max(out)                                   # 음수를 없애고, 0 ~ 1 로 scaling # [7, 7]\n",
        "#         out = F.upsample_bilinear(out.unsqueeze(0).unsqueeze(0), [224, 224])     # 4D로 바꿈\n",
        "#         return out.cuda().detach().squeeze().numpy()\n",
        "\n",
        "#     def forward_hook(self, _, input, output):\n",
        "#         self.forward_result = torch.squeeze(output)\n",
        "\n",
        "#     def backward_hook(self, _, grad_input, grad_output):\n",
        "#         self.backward_result = torch.squeeze(grad_output[0])\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "\n",
        "#     def preprocess_image(img):\n",
        "#         means = [0.485, 0.456, 0.406]\n",
        "#         stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "#         preprocessed_img = img.copy()[:, :, ::-1]\n",
        "#         for i in range(3):\n",
        "#             preprocessed_img[:, :, i] = preprocessed_img[:, :, i] - means[i]\n",
        "#             preprocessed_img[:, :, i] = preprocessed_img[:, :, i] / stds[i]\n",
        "#         preprocessed_img = np.ascontiguousarray(np.transpose(preprocessed_img, (2, 0, 1)))\n",
        "#         preprocessed_img = torch.from_numpy(preprocessed_img)\n",
        "#         preprocessed_img.unsqueeze_(0)\n",
        "#         input = preprocessed_img.requires_grad_(True)\n",
        "#         return input\n",
        "\n",
        "\n",
        "#     def show_cam_on_image(img, mask):\n",
        "\n",
        "#         # mask = (np.max(mask) - np.min(mask)) / (mask - np.min(mask))\n",
        "#         heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
        "#         heatmap = np.float32(heatmap) / 255\n",
        "#         cam = heatmap + np.float32(img)\n",
        "#         cam = cam / np.max(cam)\n",
        "#         cv2_imshow(np.uint8(255 * cam))                                          # cv2_imshow()에서 두 개의 parameter 사용 불가 --> print 부분과 imshow 부분으로 분리\n",
        "#         print(\"cam\")\n",
        "#         cv2_imshow(np.uint8(heatmap * 255))\n",
        "#         print(\"heat map\\n\\n\\n\")\n",
        "#         cv2.waitKey()\n",
        "\n",
        "\n",
        "\n",
        "#     print(model)\n",
        "#     #model.eval()\n",
        "\n",
        "#     grad_cam = GradCam(model=model, module='features', layer='30')\n",
        "#     root = '/content/gdrive/MyDrive/CUB_200_2011_repackage_class50/datasets/test'\n",
        "#     img_list = os.listdir(root)\n",
        "#     img_list = sorted(glob.glob(os.path.join(root, '*.jpg')))\n",
        "#     i = 0                                                                        # just for numbering heat maps\n",
        "#     for img_path in img_list:\n",
        "#         img = cv2.imread(img_path, 1)\n",
        "#         #img = np.float32(cv2.resize(img, (224, 224))) / 255\n",
        "#         img = np.float32(cv2.resize(img, (448, 448))) / 255\n",
        "#         input = preprocess_image(img)\n",
        "#         input = input.to(DEVICE)                                                 # input datas should be on GPU\n",
        "#         mask = grad_cam(input, None)\n",
        "#         print(\"<{}'th image>\" .format(i))\n",
        "#         show_cam_on_image(img, mask)\n",
        "#         i = i + 1                                                                # update heat maps numbering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwwqjbtGZeGU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Display\n",
        "from IPython.display import Image, display\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "model_builder = keras.applications.xception.Xception\n",
        "img_size = (299, 299)\n",
        "preprocess_input = keras.applications.xception.preprocess_input\n",
        "decode_predictions = keras.applications.xception.decode_predictions\n",
        "\n",
        "last_conv_layer_name = \"block14_sepconv2_act\"\n",
        "\n",
        "# The local path to our target image\n",
        "import random\n",
        "\n",
        "# 테스트 데이터셋에서 무작위로 이미지 선택\n",
        "random_index = random.randint(0, len(test_set) - 1)\n",
        "img_path = test_set.image_folder[random_index]\n",
        "img_path = os.path.join('/content/drive/MyDrive/datasets/test', img_path)\n",
        "display(Image(img_path))\n",
        "\n",
        "def get_img_array(img_path, size):\n",
        "    # `img` is a PIL image of size 299x299\n",
        "    img = keras.utils.load_img(img_path, target_size=size)\n",
        "    # `array` is a float32 Numpy array of shape (299, 299, 3)\n",
        "    array = keras.utils.img_to_array(img)\n",
        "    # We add a dimension to transform our array into a \"batch\"\n",
        "    # of size (1, 299, 299, 3)\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "    return array\n",
        "\n",
        "\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
        "    # First, we create a model that maps the input image to the activations\n",
        "    # of the last conv layer as well as the output predictions\n",
        "    grad_model = keras.models.Model(\n",
        "        model.inputs, [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    # Then, we compute the gradient of the top predicted class for our input image\n",
        "    # with respect to the activations of the last conv layer\n",
        "    with tf.GradientTape() as tape:\n",
        "        last_conv_layer_output, preds = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(preds[0])\n",
        "        class_channel = preds[:, pred_index]\n",
        "\n",
        "    # This is the gradient of the output neuron (top predicted or chosen)\n",
        "    # with regard to the output feature map of the last conv layer\n",
        "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
        "\n",
        "    # This is a vector where each entry is the mean intensity of the gradient\n",
        "    # over a specific feature map channel\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # We multiply each channel in the feature map array\n",
        "    # by \"how important this channel is\" with regard to the top predicted class\n",
        "    # then sum all the channels to obtain the heatmap class activation\n",
        "    last_conv_layer_output = last_conv_layer_output[0]\n",
        "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "\n",
        "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bB4foHegZuYI"
      },
      "outputs": [],
      "source": [
        "# Prepare image\n",
        "img_array = preprocess_input(get_img_array(img_path, size=img_size))\n",
        "\n",
        "# Make model\n",
        "model = model_builder(weights=\"imagenet\")\n",
        "\n",
        "# Remove last layer's softmax\n",
        "model.layers[-1].activation = None\n",
        "\n",
        "# Print what the top predicted class is\n",
        "preds = model.predict(img_array)\n",
        "print(\"Predicted:\", decode_predictions(preds, top=1)[0])\n",
        "\n",
        "# Generate class activation heatmap\n",
        "heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
        "\n",
        "# Display heatmap\n",
        "plt.matshow(heatmap)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6XMpS2aaCEq"
      },
      "outputs": [],
      "source": [
        "def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n",
        "    # Load the original image\n",
        "    img = keras.utils.load_img(img_path)\n",
        "    img = keras.utils.img_to_array(img)\n",
        "\n",
        "    # Rescale heatmap to a range 0-255\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "    # Use jet colormap to colorize heatmap\n",
        "    jet = cm.get_cmap(\"jet\")\n",
        "\n",
        "    # Use RGB values of the colormap\n",
        "    jet_colors = jet(np.arange(256))[:, :3]\n",
        "    jet_heatmap = jet_colors[heatmap]\n",
        "\n",
        "    # Create an image with RGB colorized heatmap\n",
        "    jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n",
        "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
        "    jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n",
        "\n",
        "    # Superimpose the heatmap on original image\n",
        "    superimposed_img = jet_heatmap * alpha + img\n",
        "    superimposed_img = keras.utils.array_to_img(superimposed_img)\n",
        "\n",
        "    # Save the superimposed image\n",
        "    superimposed_img.save(cam_path)\n",
        "\n",
        "    # Display Grad CAM\n",
        "    display(Image(cam_path))\n",
        "\n",
        "\n",
        "save_and_display_gradcam(img_path, heatmap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzF-6YiKlmu_"
      },
      "outputs": [],
      "source": [
        "# #keras안쓰고 토치기반으로 하는 중인데 잘 안되구 오류나서 고치는 중입니다\n",
        "\n",
        "# class GradCAM:\n",
        "#     def __init__(self, model, target_layer):\n",
        "#         self.model = model\n",
        "#         self.target_layer = target_layer\n",
        "#         self.gradients = None\n",
        "#         self.activations = None\n",
        "\n",
        "#     def save_gradient(self, grad):\n",
        "#         self.gradients = grad\n",
        "# #\n",
        "#     def save_activation(self, module, input, output):\n",
        "#         self.activations = output\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.model(x)\n",
        "\n",
        "#     def backward(self, output, class_idx):\n",
        "#         self.model.zero_grad()\n",
        "#         one_hot_output = torch.FloatTensor(1, output.size()[-1]).zero_().to(DEVICE)\n",
        "#         one_hot_output[0][class_idx] = 1\n",
        "#         one_hot_output = one_hot_output.to(DEVICE)\n",
        "\n",
        "#         self.save_gradient(one_hot_output)\n",
        "#         output.backward(gradient=one_hot_output, retain_graph=True)\n",
        "\n",
        "#     def generate(self, input_image, class_idx):\n",
        "#         self.model.eval()\n",
        "#         hook = self.target_layer.register_forward_hook(self.save_activation)\n",
        "#         output = self.forward(input_image)\n",
        "#         hook.remove()\n",
        "\n",
        "#         self.backward(output, class_idx)\n",
        "\n",
        "#         gradients = self.gradients.cpu().data.numpy()[0]\n",
        "#         activations = self.activations.cpu().data.numpy()[0]\n",
        "\n",
        "#         weights = np.mean(gradients, axis=(1, 2))\n",
        "#         cam = np.dot(activations.T, weights)\n",
        "#         cam = np.maximum(0, cam)\n",
        "#         cam = cv2.resize(cam, input_image.shape[2:])\n",
        "#         cam = cam - np.min(cam)\n",
        "#         cam = cam / np.max(cam)\n",
        "#         return cam\n",
        "\n",
        "# # Now, modify your apply_gradcam function as follows:\n",
        "\n",
        "# def apply_gradcam(image_path, model, target_layer, classes):\n",
        "#     model.eval()\n",
        "#     image = Image.open(image_path).convert('RGB')\n",
        "#     image_tensor = transforms_valtest(image).unsqueeze(0).to(DEVICE)\n",
        "#     class_idx = model(image_tensor).max(1)[-1]\n",
        "\n",
        "#     gradcam = GradCAM(model=model, target_layer=target_layer)\n",
        "#     output = gradcam.generate(image_tensor, class_idx)\n",
        "\n",
        "#     heatmap = cv2.applyColorMap(np.uint8(255 * output), cv2.COLORMAP_JET)\n",
        "#     heatmap = np.float32(heatmap) / 255\n",
        "#     cam_img = heatmap + np.float32(image_tensor.cpu().squeeze().permute(1, 2, 0))\n",
        "#     cam_img = cam_img / np.max(cam_img)\n",
        "\n",
        "#     original_image = Image.open(image_path).convert('RGB')\n",
        "#     original_image = transforms_valtest(original_image).numpy().transpose(1, 2, 0)\n",
        "\n",
        "#     plt.imshow(cam_img)\n",
        "#     plt.title(f'Predicted Class: {classes[class_idx]}')\n",
        "#     plt.show()\n",
        "\n",
        "# import random\n",
        "\n",
        "# # 테스트 데이터셋에서 무작위로 이미지 선택\n",
        "# random_index = random.randint(0, len(test_set) - 1)\n",
        "# image_path = test_set.image_folder[random_index]\n",
        "# image_path = os.path.join('/content/drive/MyDrive/datasets/test', image_path)\n",
        "\n",
        "# # GradCAM 적용 함수 정의 (앞서 정의한 GradCAM 클래스와 apply_gradcam 함수)\n",
        "# class_idx = test_set.__getitem__(random_index)[1]  # 이미지의 실제 클래스 인덱스\n",
        "# target_layers = model.layer4[-1].to(DEVICE).conv2\n",
        "# apply_gradcam(image_path, model, target_layers, classes)  # classes 변수에는 클래스 이름 목록을 할당"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1nfJHm8y2VeRSRJ1skZ0s9oUOeL_MnFSu",
      "authorship_tag": "ABX9TyNZZvAchc58VYGDg0ZCg8H+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}