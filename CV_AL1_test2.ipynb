{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yiyoungkim/yy/blob/main/CV_AL1_test2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "iaJLKOJG3nTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0b97f35-6777-4b7b-a44f-808da3548fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The goal is to solve the fine-grained image classification train the model using hyperparameter tuning, data augmentation, etc., and write a report with a detailed analysis of the results.\n",
        " - Is the problem solved in an appropriate way?\n",
        "- Are different hyperparameters and data augmentation used for training and are the results compared and analyzed?\n",
        "- Is the use of appropriate visualization tools and experiment tools for your reports?e.g., GradCAM, Weight & Biases, etc."
      ],
      "metadata": {
        "id": "PcT_ABspzPB7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "9VcoyrJVwzlS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44520833-2498-4922-9f83-e34e667bb539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: adabelief-pytorch in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from adabelief-pytorch) (2.1.0+cu118)\n",
            "Requirement already satisfied: colorama>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from adabelief-pytorch) (0.4.6)\n",
            "Requirement already satisfied: tabulate>=0.7 in /usr/local/lib/python3.10/dist-packages (from adabelief-pytorch) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->adabelief-pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.0->adabelief-pytorch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install adabelief-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "ZuomYJ5liLKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a16ab1a-e55f-47c6-c293-1ab4b39049bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from adabelief_pytorch import AdaBelief\n",
        "# 스케줄러 사용\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "import re\n",
        "\n",
        "from PIL import Image,ImageFilter,ImageEnhance\n",
        "\n",
        "from torchvision.transforms import ToTensor,Normalize, RandomHorizontalFlip, Resize\n",
        "from torch.utils.data import ConcatDataset\n",
        "\n",
        "\n",
        "### GPU Setting ###\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "print(DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "HamlO5MKZ-wk"
      },
      "outputs": [],
      "source": [
        "### Custom Dataset ###\n",
        "class CUB2011(Dataset):\n",
        "  def __init__(self, transform, mode='train'):\n",
        "    self.transform = transform\n",
        "    self.mode = mode\n",
        "\n",
        "    if self.mode == 'train':\n",
        "      self.image_folder = os.listdir('/content/drive/MyDrive/datasets/train')\n",
        "    elif self.mode == 'valid':\n",
        "      self.image_folder = os.listdir('/content/drive/MyDrive/datasets/valid')\n",
        "    elif self.mode == 'test':\n",
        "      self.image_folder = os.listdir('/content/drive/MyDrive/datasets/test')\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_folder)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = self.image_folder[idx]\n",
        "    img = Image.open(os.path.join('/content/drive/MyDrive/datasets', self.mode, img_path)).convert('RGB')\n",
        "    img = self.transform(img)\n",
        "\n",
        "\n",
        "    label = img_path.split('_')[-1].split('.')[0]\n",
        "    label = int(label)\n",
        "    return (img, label)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "om7k9Cz5uytV"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0, std=1, p=0.5):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if np.random.rand() < self.p:\n",
        "            img_array = np.array(img)\n",
        "            noise = np.random.normal(self.mean, self.std, img_array.shape)\n",
        "            noisy_image = np.clip(img_array + noise, 0, 255)  # Clip values to the range [0, 255]\n",
        "            return Image.fromarray(noisy_image.astype(np.uint8))\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std}, p={self.p})'\n",
        "\n",
        "\n",
        "class AdjustContrast(object):\n",
        "    def __init__(self, factor=1.0):\n",
        "        self.factor = factor\n",
        "\n",
        "    def __call__(self, img):\n",
        "        enhancer = ImageEnhance.Contrast(img)\n",
        "        img = enhancer.enhance(self.factor)\n",
        "        return img\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + f'(factor={self.factor})'\n",
        "\n",
        "class AdjustBrightness(object):\n",
        "    def __init__(self, factor=1.0):\n",
        "        self.factor = factor\n",
        "\n",
        "    def __call__(self, img):\n",
        "        enhancer = ImageEnhance.Brightness(img)\n",
        "        img = enhancer.enhance(self.factor)\n",
        "        return img\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + f'(factor={self.factor})'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c1qiR6PiqKL",
        "outputId": "43281815-2ad3-4364-c4a3-89cb1c598ef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of each dataset:  7080 296 298\n",
            "Loaded dataloader\n"
          ]
        }
      ],
      "source": [
        "from torchvision.transforms import RandAugment\n",
        "\n",
        "### Data Preprocessing ###\n",
        "transforms_train = transforms.Compose([transforms.Resize((448,448), Image.BICUBIC),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       ])\n",
        "transforms_test = transforms.Compose([transforms.Resize((448,448), Image.BICUBIC),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       ])\n",
        "\n",
        "#Data Augmentation ###\n",
        "transforms_train_aug = transforms.Compose([\n",
        "    transforms.Resize((448, 448), Image.BICUBIC),\n",
        "    RandAugment(5,3),  # Apply RandAugment\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "transforms_train_g_v = transforms.Compose([\n",
        "    transforms.Resize((448, 448), Image.BICUBIC),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.RandomVerticalFlip(0.5),\n",
        "    transforms.RandomRotation(30),\n",
        "    AddGaussianNoise(mean=0, std=25, p=0.5),  # 가우시안 노이즈를 추가합니다.\n",
        "    AdjustContrast(factor=2.0),  # 대비를 조절합니다.\n",
        "    AdjustBrightness(factor=1.5),  # 밝기를 조절합니다.\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_set = CUB2011(mode='train',\n",
        "                    transform=transforms_train)\n",
        "val_set = CUB2011(mode='valid',\n",
        "                  transform=transforms_test)\n",
        "test_set = CUB2011(mode='test',\n",
        "                  transform=transforms_test)\n",
        "\n",
        "# 데이터 증강을 위해 원래 데이터를 복사하고 추가\n",
        "train_set_augmented = CUB2011(mode='train', transform=transforms_train_aug)\n",
        "train_set_augmented2 = CUB2011(mode='train',transform=transforms_train_g_v)\n",
        "\n",
        "# 두 데이터셋을 연결하여 새로운 훈련 데이터셋 생성\n",
        "train_set_combined = ConcatDataset([train_set, train_set_augmented, train_set_augmented2])\n",
        "\n",
        "print('Num of each dataset: ',len(train_set_combined),len(val_set),len(test_set))\n",
        "\n",
        "# Dataloader class는 bath기반의 딥러닝모델 학습을 위해서 mini batch를 만들어주는 역할을 한다\n",
        "# dataloader를 통해 dataset의 전체 데이터가 batch size로 나뉘게 된다\n",
        "train_loader = DataLoader(train_set_combined,batch_size=BATCH_SIZE,shuffle=True)\n",
        "val_loader = DataLoader(val_set,batch_size=BATCH_SIZE,shuffle=False)\n",
        "test_loader = DataLoader(test_set,batch_size=BATCH_SIZE,shuffle=False)\n",
        "\n",
        "print(\"Loaded dataloader\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BCFpbfxxlNe",
        "outputId": "bfa44cdb-2058-48d1-dd73-724b1e92feb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: adabelief-pytorch in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from adabelief-pytorch) (2.1.0+cu118)\n",
            "Requirement already satisfied: colorama>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from adabelief-pytorch) (0.4.6)\n",
            "Requirement already satisfied: tabulate>=0.7 in /usr/local/lib/python3.10/dist-packages (from adabelief-pytorch) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->adabelief-pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.0->adabelief-pytorch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade adabelief-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1E8YNNbiwOC",
        "outputId": "bccd8025-08dc-44f2-ee7a-cc615b7a8c52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
            "\u001b[31mModifications to default arguments:\n",
            "\u001b[31m                           eps  weight_decouple    rectify\n",
            "-----------------------  -----  -----------------  ---------\n",
            "adabelief-pytorch=0.0.5  1e-08  False              False\n",
            ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
            "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
            "----------------------------------------------------------  ----------------------------------------------\n",
            "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
            "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
            "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
            "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
            "\u001b[0m\n",
            "Weight decoupling enabled in AdaBelief\n",
            "Rectification enabled in AdaBelief\n",
            "Created a learning model and optimizer\n"
          ]
        }
      ],
      "source": [
        "### Model / Optimizer ###\n",
        "EPOCH = 30\n",
        "\n",
        "lr = 0.001\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "### Tranfer Learning ###\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features,50)\n",
        "model.to(DEVICE)\n",
        "\n",
        "optimizer = AdaBelief(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-16, weight_decouple=True, rectify=True)\n",
        "\n",
        "# 코사인 앤니얼링 스케줄러 초기화\n",
        "dataset_size = len(train_set_combined)  # 훈련 데이터셋 크기\n",
        "\n",
        "T_max = dataset_size / BATCH_SIZE  # 한 주기의 에폭 수\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=T_max, eta_min=0.001)\n",
        "\n",
        "print(\"Created a learning model and optimizer\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "mTQsnbhbzm7C"
      },
      "outputs": [],
      "source": [
        "### Train/Evaluation ###\n",
        "def train(model,train_loader,optimizer,epoch):\n",
        "  model.train()\n",
        "  for i,(image,target) in enumerate(train_loader):\n",
        "    image,target = image.to(DEVICE),target.to(DEVICE)\n",
        "    output = model(image)\n",
        "    optimizer.zero_grad()\n",
        "    # loss func을 어떤 것을 사용할 것인지?\n",
        "    train_loss = F.cross_entropy(output,target).to(DEVICE)\n",
        "\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 스케줄러 업데이트\n",
        "    scheduler.step()\n",
        "\n",
        "    if i%10 ==0:\n",
        "      print(\n",
        "          f'Train Epoch: {epoch} [{i}/{len(train_loader)}]\\tloss: {train_loss.item():6f}')\n",
        "\n",
        "  return train_loss\n",
        "\n",
        "def evaluate(model,val_loader):\n",
        "  model.eval()\n",
        "  eval_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for i,(image,target) in enumerate(val_loader):\n",
        "      image,target = image.to(DEVICE),target.to(DEVICE)\n",
        "      output = model(image)\n",
        "\n",
        "      eval_loss += F.cross_entropy(output,target, reduction='sum').item()\n",
        "      pred = output.max(1,keepdim=True)[1]\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  eval_loss /= len(val_loader.dataset)\n",
        "  eval_accuracy = 100*correct / len(val_loader.dataset)\n",
        "  return eval_loss,eval_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yZ2smo7ucLY0",
        "outputId": "4e4462a1-9940-453e-adb8-53f682039913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/111]\tloss: 4.077477\n",
            "Train Epoch: 0 [10/111]\tloss: 3.968934\n",
            "Train Epoch: 0 [20/111]\tloss: 3.707734\n",
            "Train Epoch: 0 [30/111]\tloss: 3.425055\n",
            "Train Epoch: 0 [40/111]\tloss: 2.870303\n",
            "Train Epoch: 0 [50/111]\tloss: 2.687671\n",
            "Train Epoch: 0 [60/111]\tloss: 2.515092\n",
            "Train Epoch: 0 [70/111]\tloss: 2.062613\n",
            "Train Epoch: 0 [80/111]\tloss: 1.863353\n",
            "Train Epoch: 0 [90/111]\tloss: 1.826660\n",
            "Train Epoch: 0 [100/111]\tloss: 1.474455\n",
            "Train Epoch: 0 [110/111]\tloss: 1.599238\n",
            "[0]Validation Loss: 1.2825,Accuracy: 74.6622%\n",
            "Train Epoch: 1 [0/111]\tloss: 1.151300\n",
            "Train Epoch: 1 [10/111]\tloss: 0.892151\n",
            "Train Epoch: 1 [20/111]\tloss: 0.977030\n",
            "Train Epoch: 1 [30/111]\tloss: 0.942634\n",
            "Train Epoch: 1 [40/111]\tloss: 0.804202\n",
            "Train Epoch: 1 [50/111]\tloss: 0.906679\n",
            "Train Epoch: 1 [60/111]\tloss: 0.855747\n",
            "Train Epoch: 1 [70/111]\tloss: 0.708922\n",
            "Train Epoch: 1 [80/111]\tloss: 0.890123\n",
            "Train Epoch: 1 [90/111]\tloss: 0.726143\n",
            "Train Epoch: 1 [100/111]\tloss: 0.665690\n",
            "Train Epoch: 1 [110/111]\tloss: 0.627588\n",
            "[1]Validation Loss: 1.4811,Accuracy: 63.5135%\n",
            "Train Epoch: 2 [0/111]\tloss: 0.815871\n",
            "Train Epoch: 2 [10/111]\tloss: 0.358256\n",
            "Train Epoch: 2 [20/111]\tloss: 0.451764\n",
            "Train Epoch: 2 [30/111]\tloss: 0.427839\n",
            "Train Epoch: 2 [40/111]\tloss: 0.278271\n",
            "Train Epoch: 2 [50/111]\tloss: 0.347569\n",
            "Train Epoch: 2 [60/111]\tloss: 0.629788\n",
            "Train Epoch: 2 [70/111]\tloss: 0.407168\n",
            "Train Epoch: 2 [80/111]\tloss: 0.340472\n",
            "Train Epoch: 2 [90/111]\tloss: 0.279893\n",
            "Train Epoch: 2 [100/111]\tloss: 0.417504\n",
            "Train Epoch: 2 [110/111]\tloss: 0.362813\n",
            "[2]Validation Loss: 0.8050,Accuracy: 76.3514%\n",
            "Train Epoch: 3 [0/111]\tloss: 0.272950\n",
            "Train Epoch: 3 [10/111]\tloss: 0.384268\n",
            "Train Epoch: 3 [20/111]\tloss: 0.477545\n",
            "Train Epoch: 3 [30/111]\tloss: 0.394211\n",
            "Train Epoch: 3 [40/111]\tloss: 0.222774\n",
            "Train Epoch: 3 [50/111]\tloss: 0.385302\n",
            "Train Epoch: 3 [60/111]\tloss: 0.355046\n",
            "Train Epoch: 3 [70/111]\tloss: 0.356460\n",
            "Train Epoch: 3 [80/111]\tloss: 0.282850\n",
            "Train Epoch: 3 [90/111]\tloss: 0.297277\n",
            "Train Epoch: 3 [100/111]\tloss: 0.496484\n",
            "Train Epoch: 3 [110/111]\tloss: 0.323988\n",
            "[3]Validation Loss: 0.8159,Accuracy: 80.4054%\n",
            "Train Epoch: 4 [0/111]\tloss: 0.261168\n",
            "Train Epoch: 4 [10/111]\tloss: 0.297196\n",
            "Train Epoch: 4 [20/111]\tloss: 0.251035\n",
            "Train Epoch: 4 [30/111]\tloss: 0.310439\n",
            "Train Epoch: 4 [40/111]\tloss: 0.173257\n",
            "Train Epoch: 4 [50/111]\tloss: 0.330013\n",
            "Train Epoch: 4 [60/111]\tloss: 0.183988\n",
            "Train Epoch: 4 [70/111]\tloss: 0.277336\n",
            "Train Epoch: 4 [80/111]\tloss: 0.383907\n",
            "Train Epoch: 4 [90/111]\tloss: 0.309103\n",
            "Train Epoch: 4 [100/111]\tloss: 0.389323\n",
            "Train Epoch: 4 [110/111]\tloss: 0.319556\n",
            "[4]Validation Loss: 1.2574,Accuracy: 67.2297%\n",
            "Train Epoch: 5 [0/111]\tloss: 0.200293\n",
            "Train Epoch: 5 [10/111]\tloss: 0.427891\n",
            "Train Epoch: 5 [20/111]\tloss: 0.239153\n",
            "Train Epoch: 5 [30/111]\tloss: 0.297175\n",
            "Train Epoch: 5 [40/111]\tloss: 0.284300\n",
            "Train Epoch: 5 [50/111]\tloss: 0.119959\n",
            "Train Epoch: 5 [60/111]\tloss: 0.299441\n",
            "Train Epoch: 5 [70/111]\tloss: 0.397826\n",
            "Train Epoch: 5 [80/111]\tloss: 0.338209\n",
            "Train Epoch: 5 [90/111]\tloss: 0.283429\n",
            "Train Epoch: 5 [100/111]\tloss: 0.237569\n",
            "Train Epoch: 5 [110/111]\tloss: 0.465180\n",
            "[5]Validation Loss: 0.8542,Accuracy: 74.3243%\n",
            "Train Epoch: 6 [0/111]\tloss: 0.113524\n",
            "Train Epoch: 6 [10/111]\tloss: 0.268624\n",
            "Train Epoch: 6 [20/111]\tloss: 0.251268\n",
            "Train Epoch: 6 [30/111]\tloss: 0.252058\n",
            "Train Epoch: 6 [40/111]\tloss: 0.432621\n",
            "Train Epoch: 6 [50/111]\tloss: 0.290377\n",
            "Train Epoch: 6 [60/111]\tloss: 0.304912\n",
            "Train Epoch: 6 [70/111]\tloss: 0.169371\n",
            "Train Epoch: 6 [80/111]\tloss: 0.186575\n",
            "Train Epoch: 6 [90/111]\tloss: 0.253823\n",
            "Train Epoch: 6 [100/111]\tloss: 0.182086\n",
            "Train Epoch: 6 [110/111]\tloss: 0.115868\n",
            "[6]Validation Loss: 0.7754,Accuracy: 76.6892%\n",
            "Train Epoch: 7 [0/111]\tloss: 0.378741\n",
            "Train Epoch: 7 [10/111]\tloss: 0.198715\n",
            "Train Epoch: 7 [20/111]\tloss: 0.187869\n",
            "Train Epoch: 7 [30/111]\tloss: 0.293278\n",
            "Train Epoch: 7 [40/111]\tloss: 0.393536\n",
            "Train Epoch: 7 [50/111]\tloss: 0.187960\n",
            "Train Epoch: 7 [60/111]\tloss: 0.162575\n",
            "Train Epoch: 7 [70/111]\tloss: 0.196389\n",
            "Train Epoch: 7 [80/111]\tloss: 0.238676\n",
            "Train Epoch: 7 [90/111]\tloss: 0.289963\n",
            "Train Epoch: 7 [100/111]\tloss: 0.217477\n",
            "Train Epoch: 7 [110/111]\tloss: 0.430477\n",
            "[7]Validation Loss: 1.0403,Accuracy: 72.9730%\n",
            "Train Epoch: 8 [0/111]\tloss: 0.190493\n",
            "Train Epoch: 8 [10/111]\tloss: 0.190956\n",
            "Train Epoch: 8 [20/111]\tloss: 0.333567\n",
            "Train Epoch: 8 [30/111]\tloss: 0.156677\n",
            "Train Epoch: 8 [40/111]\tloss: 0.144075\n",
            "Train Epoch: 8 [50/111]\tloss: 0.184784\n",
            "Train Epoch: 8 [60/111]\tloss: 0.125071\n",
            "Train Epoch: 8 [70/111]\tloss: 0.139606\n",
            "Train Epoch: 8 [80/111]\tloss: 0.511842\n",
            "Train Epoch: 8 [90/111]\tloss: 0.206929\n",
            "Train Epoch: 8 [100/111]\tloss: 0.224807\n",
            "Train Epoch: 8 [110/111]\tloss: 0.385894\n",
            "[8]Validation Loss: 1.5495,Accuracy: 62.5000%\n",
            "Train Epoch: 9 [0/111]\tloss: 0.105793\n",
            "Train Epoch: 9 [10/111]\tloss: 0.258290\n",
            "Train Epoch: 9 [20/111]\tloss: 0.212272\n",
            "Train Epoch: 9 [30/111]\tloss: 0.192917\n",
            "Train Epoch: 9 [40/111]\tloss: 0.153603\n",
            "Train Epoch: 9 [50/111]\tloss: 0.087298\n",
            "Train Epoch: 9 [60/111]\tloss: 0.374506\n",
            "Train Epoch: 9 [70/111]\tloss: 0.219140\n",
            "Train Epoch: 9 [80/111]\tloss: 0.249815\n",
            "Train Epoch: 9 [90/111]\tloss: 0.265827\n",
            "Train Epoch: 9 [100/111]\tloss: 0.133969\n",
            "Train Epoch: 9 [110/111]\tloss: 0.205803\n",
            "[9]Validation Loss: 0.8646,Accuracy: 79.0541%\n",
            "Train Epoch: 10 [0/111]\tloss: 0.077364\n",
            "Train Epoch: 10 [10/111]\tloss: 0.242424\n",
            "Train Epoch: 10 [20/111]\tloss: 0.147455\n",
            "Train Epoch: 10 [30/111]\tloss: 0.099942\n",
            "Train Epoch: 10 [40/111]\tloss: 0.116539\n",
            "Train Epoch: 10 [50/111]\tloss: 0.176504\n",
            "Train Epoch: 10 [60/111]\tloss: 0.170983\n",
            "Train Epoch: 10 [70/111]\tloss: 0.298785\n",
            "Train Epoch: 10 [80/111]\tloss: 0.310023\n",
            "Train Epoch: 10 [90/111]\tloss: 0.342856\n",
            "Train Epoch: 10 [100/111]\tloss: 0.433535\n",
            "Train Epoch: 10 [110/111]\tloss: 0.171964\n",
            "[10]Validation Loss: 1.2663,Accuracy: 69.9324%\n",
            "Train Epoch: 11 [0/111]\tloss: 0.254106\n",
            "Train Epoch: 11 [10/111]\tloss: 0.178616\n",
            "Train Epoch: 11 [20/111]\tloss: 0.349736\n",
            "Train Epoch: 11 [30/111]\tloss: 0.148055\n",
            "Train Epoch: 11 [40/111]\tloss: 0.210942\n",
            "Train Epoch: 11 [50/111]\tloss: 0.375191\n",
            "Train Epoch: 11 [60/111]\tloss: 0.345943\n",
            "Train Epoch: 11 [70/111]\tloss: 0.104668\n",
            "Train Epoch: 11 [80/111]\tloss: 0.244526\n",
            "Train Epoch: 11 [90/111]\tloss: 0.170875\n",
            "Train Epoch: 11 [100/111]\tloss: 0.273148\n",
            "Train Epoch: 11 [110/111]\tloss: 0.261514\n",
            "[11]Validation Loss: 0.7181,Accuracy: 81.7568%\n",
            "Train Epoch: 12 [0/111]\tloss: 0.165174\n",
            "Train Epoch: 12 [10/111]\tloss: 0.216492\n",
            "Train Epoch: 12 [20/111]\tloss: 0.088981\n",
            "Train Epoch: 12 [30/111]\tloss: 0.176373\n",
            "Train Epoch: 12 [40/111]\tloss: 0.203979\n",
            "Train Epoch: 12 [50/111]\tloss: 0.139045\n",
            "Train Epoch: 12 [60/111]\tloss: 0.122468\n",
            "Train Epoch: 12 [70/111]\tloss: 0.054138\n",
            "Train Epoch: 12 [80/111]\tloss: 0.129366\n",
            "Train Epoch: 12 [90/111]\tloss: 0.205793\n",
            "Train Epoch: 12 [100/111]\tloss: 0.135062\n",
            "Train Epoch: 12 [110/111]\tloss: 0.091648\n",
            "[12]Validation Loss: 0.9380,Accuracy: 77.7027%\n",
            "Train Epoch: 13 [0/111]\tloss: 0.073239\n",
            "Train Epoch: 13 [10/111]\tloss: 0.057336\n",
            "Train Epoch: 13 [20/111]\tloss: 0.022019\n",
            "Train Epoch: 13 [30/111]\tloss: 0.150741\n",
            "Train Epoch: 13 [40/111]\tloss: 0.165939\n",
            "Train Epoch: 13 [50/111]\tloss: 0.087853\n",
            "Train Epoch: 13 [60/111]\tloss: 0.097086\n",
            "Train Epoch: 13 [70/111]\tloss: 0.193628\n",
            "Train Epoch: 13 [80/111]\tloss: 0.159648\n",
            "Train Epoch: 13 [90/111]\tloss: 0.186438\n",
            "Train Epoch: 13 [100/111]\tloss: 0.060459\n",
            "Train Epoch: 13 [110/111]\tloss: 0.279378\n",
            "[13]Validation Loss: 1.1768,Accuracy: 75.6757%\n",
            "Train Epoch: 14 [0/111]\tloss: 0.196708\n",
            "Train Epoch: 14 [10/111]\tloss: 0.136805\n",
            "Train Epoch: 14 [20/111]\tloss: 0.103620\n",
            "Train Epoch: 14 [30/111]\tloss: 0.151867\n",
            "Train Epoch: 14 [40/111]\tloss: 0.187719\n",
            "Train Epoch: 14 [50/111]\tloss: 0.140517\n",
            "Train Epoch: 14 [60/111]\tloss: 0.278762\n",
            "Train Epoch: 14 [70/111]\tloss: 0.113634\n",
            "Train Epoch: 14 [80/111]\tloss: 0.069612\n",
            "Train Epoch: 14 [90/111]\tloss: 0.155168\n",
            "Train Epoch: 14 [100/111]\tloss: 0.357245\n",
            "Train Epoch: 14 [110/111]\tloss: 0.110530\n",
            "[14]Validation Loss: 1.1526,Accuracy: 70.9459%\n",
            "Train Epoch: 15 [0/111]\tloss: 0.111351\n",
            "Train Epoch: 15 [10/111]\tloss: 0.127087\n",
            "Train Epoch: 15 [20/111]\tloss: 0.175153\n",
            "Train Epoch: 15 [30/111]\tloss: 0.200586\n",
            "Train Epoch: 15 [40/111]\tloss: 0.099874\n",
            "Train Epoch: 15 [50/111]\tloss: 0.211911\n",
            "Train Epoch: 15 [60/111]\tloss: 0.121946\n",
            "Train Epoch: 15 [70/111]\tloss: 0.121645\n",
            "Train Epoch: 15 [80/111]\tloss: 0.127711\n",
            "Train Epoch: 15 [90/111]\tloss: 0.119871\n",
            "Train Epoch: 15 [100/111]\tloss: 0.140431\n",
            "Train Epoch: 15 [110/111]\tloss: 0.147538\n",
            "[15]Validation Loss: 1.0106,Accuracy: 72.6351%\n",
            "Train Epoch: 16 [0/111]\tloss: 0.288839\n",
            "Train Epoch: 16 [10/111]\tloss: 0.137787\n",
            "Train Epoch: 16 [20/111]\tloss: 0.085828\n",
            "Train Epoch: 16 [30/111]\tloss: 0.136280\n",
            "Train Epoch: 16 [40/111]\tloss: 0.178256\n",
            "Train Epoch: 16 [50/111]\tloss: 0.162148\n",
            "Train Epoch: 16 [60/111]\tloss: 0.132520\n",
            "Train Epoch: 16 [70/111]\tloss: 0.138117\n",
            "Train Epoch: 16 [80/111]\tloss: 0.120311\n",
            "Train Epoch: 16 [90/111]\tloss: 0.196323\n",
            "Train Epoch: 16 [100/111]\tloss: 0.111987\n",
            "Train Epoch: 16 [110/111]\tloss: 0.104920\n",
            "[16]Validation Loss: 0.8475,Accuracy: 77.7027%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-77760d62ad76>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-49c5f3ad07cd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0msample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-c80199ca10fa>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_folder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/datasets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/datasets/train/72_21.jpg'"
          ]
        }
      ],
      "source": [
        "#### Mian ###\n",
        "start = time.time()\n",
        "best = 0\n",
        "\n",
        "train_losses = []  # 훈련 손실을 저장할 목록\n",
        "val_losses = []    # 검증 손실을 저장할 목록\n",
        "val_accuracys = []\n",
        "\n",
        "# 디렉토리 생성\n",
        "os.makedirs('./best_model', exist_ok=True)\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "  train_loss = train(model,train_loader,optimizer,epoch)\n",
        "  val_loss,val_accuracy = evaluate(model,val_loader)\n",
        "\n",
        "  # Save best model\n",
        "  if val_accuracy > best:\n",
        "    best = val_accuracy\n",
        "    torch.save(model.state_dict(),\"./best_model.pth\")\n",
        "\n",
        "  print(f\"[{epoch}]Validation Loss: {val_loss:.4f},Accuracy: {val_accuracy:.4f}%\")\n",
        "\n",
        "# Test result\n",
        "test_loss,test_accuracy = evaluate(model,test_loader)\n",
        "print(f'[FINAL] Test Loss: {test_loss:.4f},Accuracy: {test_accuracy:.4f}%')\n",
        "\n",
        "end = time.time()\n",
        "elasped_time = end - start\n",
        "\n",
        "print(\"Best Accuracy: \",best)\n",
        "print(\n",
        "    f\"Elasped Time: {int(elasped_time/3600)}h, {int(elasped_time/60)}m, {int(elasped_time%60)}s\")\n",
        "print(\n",
        "    f\"time: {int(elasped_time/3600)}h, {int(elasped_time/60)}m, {int(elasped_time%60)}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AtJ_lNacFqm"
      },
      "outputs": [],
      "source": [
        "# 이제 손실 값을 플로팅합니다.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# train_loss와 val_loss 텐서를 CPU로 이동하고 NumPy 배열로 변환\n",
        "train_losses = [loss.cpu().detach().numpy() if isinstance(loss, torch.Tensor) else loss for loss in train_losses]\n",
        "val_losses = [loss.cpu().detach().numpy() if isinstance(loss, torch.Tensor) else loss for loss in val_losses]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, EPOCH + 1), train_losses, label='train_loss', marker='o')\n",
        "plt.plot(range(1, EPOCH + 1), val_losses, label='val_loss', marker='o')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "\n",
        "plt.xlabel('EPOCH')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "val_accuracys = [acc.cpu().detach().numpy() if isinstance(acc, torch.Tensor) else acc for acc in val_accuracys]\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, EPOCH + 1), val_accuracys, label='val_accuracy', marker='o')\n",
        "plt.title('Validation Accuracy Over Epochs')\n",
        "\n",
        "plt.xlabel('EPOCH')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbSDhRMYYvVy"
      },
      "outputs": [],
      "source": [
        "# !pip install git+https://github.com/jacobgil/pytorch-grad-cam.git\n",
        "\n",
        "# import copy\n",
        "# from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
        "# from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "# from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "# import numpy as np\n",
        "# from PIL import Image\n",
        "\n",
        "# import torchvision\n",
        "# #\n",
        "# # Pick up layers for visualization\n",
        "# target_layers = [model.layer4[-1]]\n",
        "# grad_cam = GradCam(model = model, target_layers, use_cuda=False)\n",
        "\n",
        "# # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
        "# grayscale_cam = cam(input_tensor=input_tensor)\n",
        "\n",
        "# # In this example grayscale_cam has only one image in the batch:\n",
        "# grayscale_cam = grayscale_cam[0, :]\n",
        "# visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDscRu5Ndahh"
      },
      "outputs": [],
      "source": [
        "# # gradCAM\n",
        "# import cv2\n",
        "# from google.colab.patches import cv2_imshow                                      # cv2.imshow() should be replaced to cv2_imshow() in google.colab\n",
        "# import glob\n",
        "# import numpy as np\n",
        "\n",
        "\n",
        "# class GradCam(nn.Module):\n",
        "#     def __init__(self, model, module, layer):\n",
        "#         super().__init__()\n",
        "#         self.model = model\n",
        "#         self.module = module\n",
        "#         self.layer = layer\n",
        "#         self.register_hooks()\n",
        "\n",
        "#     def register_hooks(self):\n",
        "#         for modue_name, module in self.model._modules.items():\n",
        "#             if modue_name == self.module:\n",
        "#                 for layer_name, module in module._modules.items():\n",
        "#                     if layer_name == self.layer:\n",
        "#                         module.register_forward_hook(self.forward_hook)\n",
        "#                         module.register_backward_hook(self.backward_hook)\n",
        "\n",
        "#     def forward(self, input, target_index):\n",
        "#         outs = self.model(input)\n",
        "#         outs = outs.squeeze()                                                    # [1, num_classes]  --> [num_classes]\n",
        "#         outs = outs.to(DEVICE)\n",
        "\n",
        "#         # 가장 큰 값을 가지는 것을 target index 로 사용\n",
        "#         if target_index is None:\n",
        "#             target_index = outs.argmax()\n",
        "#             target_index = target_index.to(DEVICE)\n",
        "\n",
        "#         outs[target_index].backward(retain_graph=True)\n",
        "#         a_k = torch.mean(self.backward_result, dim=(1, 2), keepdim=True)         # [512, 1, 1]\n",
        "#         out = torch.sum(a_k * self.forward_result, dim=0).cuda()                 # [512, 7, 7] * [512, 1, 1]\n",
        "#         out = torch.relu(out) / torch.max(out)                                   # 음수를 없애고, 0 ~ 1 로 scaling # [7, 7]\n",
        "#         out = F.upsample_bilinear(out.unsqueeze(0).unsqueeze(0), [224, 224])     # 4D로 바꿈\n",
        "#         return out.cuda().detach().squeeze().numpy()\n",
        "\n",
        "#     def forward_hook(self, _, input, output):\n",
        "#         self.forward_result = torch.squeeze(output)\n",
        "\n",
        "#     def backward_hook(self, _, grad_input, grad_output):\n",
        "#         self.backward_result = torch.squeeze(grad_output[0])\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "\n",
        "#     def preprocess_image(img):\n",
        "#         means = [0.485, 0.456, 0.406]\n",
        "#         stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "#         preprocessed_img = img.copy()[:, :, ::-1]\n",
        "#         for i in range(3):\n",
        "#             preprocessed_img[:, :, i] = preprocessed_img[:, :, i] - means[i]\n",
        "#             preprocessed_img[:, :, i] = preprocessed_img[:, :, i] / stds[i]\n",
        "#         preprocessed_img = np.ascontiguousarray(np.transpose(preprocessed_img, (2, 0, 1)))\n",
        "#         preprocessed_img = torch.from_numpy(preprocessed_img)\n",
        "#         preprocessed_img.unsqueeze_(0)\n",
        "#         input = preprocessed_img.requires_grad_(True)\n",
        "#         return input\n",
        "\n",
        "\n",
        "#     def show_cam_on_image(img, mask):\n",
        "\n",
        "#         # mask = (np.max(mask) - np.min(mask)) / (mask - np.min(mask))\n",
        "#         heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
        "#         heatmap = np.float32(heatmap) / 255\n",
        "#         cam = heatmap + np.float32(img)\n",
        "#         cam = cam / np.max(cam)\n",
        "#         cv2_imshow(np.uint8(255 * cam))                                          # cv2_imshow()에서 두 개의 parameter 사용 불가 --> print 부분과 imshow 부분으로 분리\n",
        "#         print(\"cam\")\n",
        "#         cv2_imshow(np.uint8(heatmap * 255))\n",
        "#         print(\"heat map\\n\\n\\n\")\n",
        "#         cv2.waitKey()\n",
        "\n",
        "\n",
        "\n",
        "#     print(model)\n",
        "#     #model.eval()\n",
        "\n",
        "#     grad_cam = GradCam(model=model, module='features', layer='30')\n",
        "#     root = '/content/gdrive/MyDrive/CUB_200_2011_repackage_class50/datasets/test'\n",
        "#     img_list = os.listdir(root)\n",
        "#     img_list = sorted(glob.glob(os.path.join(root, '*.jpg')))\n",
        "#     i = 0                                                                        # just for numbering heat maps\n",
        "#     for img_path in img_list:\n",
        "#         img = cv2.imread(img_path, 1)\n",
        "#         #img = np.float32(cv2.resize(img, (224, 224))) / 255\n",
        "#         img = np.float32(cv2.resize(img, (448, 448))) / 255\n",
        "#         input = preprocess_image(img)\n",
        "#         input = input.to(DEVICE)                                                 # input datas should be on GPU\n",
        "#         mask = grad_cam(input, None)\n",
        "#         print(\"<{}'th image>\" .format(i))\n",
        "#         show_cam_on_image(img, mask)\n",
        "#         i = i + 1                                                                # update heat maps numbering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwwqjbtGZeGU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Display\n",
        "from IPython.display import Image, display\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "model_builder = keras.applications.xception.Xception\n",
        "img_size = (299, 299)\n",
        "preprocess_input = keras.applications.xception.preprocess_input\n",
        "decode_predictions = keras.applications.xception.decode_predictions\n",
        "\n",
        "last_conv_layer_name = \"block14_sepconv2_act\"\n",
        "\n",
        "# The local path to our target image\n",
        "import random\n",
        "\n",
        "# 테스트 데이터셋에서 무작위로 이미지 선택\n",
        "random_index = random.randint(0, len(test_set) - 1)\n",
        "img_path = test_set.image_folder[random_index]\n",
        "img_path = os.path.join('/content/drive/MyDrive/datasets/test', image_path)\n",
        "display(Image(img_path))\n",
        "\n",
        "def get_img_array(img_path, size):\n",
        "    # `img` is a PIL image of size 299x299\n",
        "    img = keras.utils.load_img(img_path, target_size=size)\n",
        "    # `array` is a float32 Numpy array of shape (299, 299, 3)\n",
        "    array = keras.utils.img_to_array(img)\n",
        "    # We add a dimension to transform our array into a \"batch\"\n",
        "    # of size (1, 299, 299, 3)\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "    return array\n",
        "\n",
        "\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
        "    # First, we create a model that maps the input image to the activations\n",
        "    # of the last conv layer as well as the output predictions\n",
        "    grad_model = keras.models.Model(\n",
        "        model.inputs, [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    # Then, we compute the gradient of the top predicted class for our input image\n",
        "    # with respect to the activations of the last conv layer\n",
        "    with tf.GradientTape() as tape:\n",
        "        last_conv_layer_output, preds = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(preds[0])\n",
        "        class_channel = preds[:, pred_index]\n",
        "\n",
        "    # This is the gradient of the output neuron (top predicted or chosen)\n",
        "    # with regard to the output feature map of the last conv layer\n",
        "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
        "\n",
        "    # This is a vector where each entry is the mean intensity of the gradient\n",
        "    # over a specific feature map channel\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # We multiply each channel in the feature map array\n",
        "    # by \"how important this channel is\" with regard to the top predicted class\n",
        "    # then sum all the channels to obtain the heatmap class activation\n",
        "    last_conv_layer_output = last_conv_layer_output[0]\n",
        "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "\n",
        "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "    #귀여워..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bB4foHegZuYI"
      },
      "outputs": [],
      "source": [
        "# Prepare image\n",
        "img_array = preprocess_input(get_img_array(img_path, size=img_size))\n",
        "\n",
        "# Make model\n",
        "model = model_builder(weights=\"imagenet\")\n",
        "\n",
        "# Remove last layer's softmax\n",
        "model.layers[-1].activation = None\n",
        "\n",
        "# Print what the top predicted class is\n",
        "preds = model.predict(img_array)\n",
        "print(\"Predicted:\", decode_predictions(preds, top=1)[0])\n",
        "\n",
        "# Generate class activation heatmap\n",
        "heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
        "\n",
        "# Display heatmap\n",
        "plt.matshow(heatmap)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6XMpS2aaCEq"
      },
      "outputs": [],
      "source": [
        "def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n",
        "    # Load the original image\n",
        "    img = keras.utils.load_img(img_path)\n",
        "    img = keras.utils.img_to_array(img)\n",
        "\n",
        "    # Rescale heatmap to a range 0-255\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "    # Use jet colormap to colorize heatmap\n",
        "    jet = cm.get_cmap(\"jet\")\n",
        "\n",
        "    # Use RGB values of the colormap\n",
        "    jet_colors = jet(np.arange(256))[:, :3]\n",
        "    jet_heatmap = jet_colors[heatmap]\n",
        "\n",
        "    # Create an image with RGB colorized heatmap\n",
        "    jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n",
        "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
        "    jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n",
        "\n",
        "    # Superimpose the heatmap on original image\n",
        "    superimposed_img = jet_heatmap * alpha + img\n",
        "    superimposed_img = keras.utils.array_to_img(superimposed_img)\n",
        "\n",
        "    # Save the superimposed image\n",
        "    superimposed_img.save(cam_path)\n",
        "\n",
        "    # Display Grad CAM\n",
        "    display(Image(cam_path))\n",
        "\n",
        "\n",
        "save_and_display_gradcam(img_path, heatmap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzF-6YiKlmu_"
      },
      "outputs": [],
      "source": [
        "#keras안쓰고 토치기반으로 하는 중인데 잘 안되구 오류나서 고치는 중입니다\n",
        "\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "\n",
        "    def save_gradient(self, grad):\n",
        "        self.gradients = grad\n",
        "\n",
        "    def save_activation(self, module, input, output):\n",
        "        self.activations = output\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def backward(self, output, class_idx):\n",
        "        self.model.zero_grad()\n",
        "        one_hot_output = torch.FloatTensor(1, output.size()[-1]).zero_().to(DEVICE)\n",
        "        one_hot_output[0][class_idx] = 1\n",
        "        one_hot_output = one_hot_output.to(DEVICE)\n",
        "\n",
        "        self.save_gradient(one_hot_output)\n",
        "        output.backward(gradient=one_hot_output, retain_graph=True)\n",
        "\n",
        "    def generate(self, input_image, class_idx):\n",
        "        self.model.eval()\n",
        "        hook = self.target_layer.register_forward_hook(self.save_activation)\n",
        "        output = self.forward(input_image)\n",
        "        hook.remove()\n",
        "\n",
        "        self.backward(output, class_idx)\n",
        "\n",
        "        gradients = self.gradients.cpu().data.numpy()[0]\n",
        "        activations = self.activations.cpu().data.numpy()[0]\n",
        "\n",
        "        weights = np.mean(gradients, axis=(1, 2))\n",
        "        cam = np.dot(activations.T, weights)\n",
        "        cam = np.maximum(0, cam)\n",
        "        cam = cv2.resize(cam, input_image.shape[2:])\n",
        "        cam = cam - np.min(cam)\n",
        "        cam = cam / np.max(cam)\n",
        "        return cam\n",
        "\n",
        "# Now, modify your apply_gradcam function as follows:\n",
        "\n",
        "def apply_gradcam(image_path, model, target_layer, classes):\n",
        "    model.eval()\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transforms_valtest(image).unsqueeze(0).to(DEVICE)\n",
        "    class_idx = model(image_tensor).max(1)[-1]\n",
        "\n",
        "    gradcam = GradCAM(model=model, target_layer=target_layer)\n",
        "    output = gradcam.generate(image_tensor, class_idx)\n",
        "\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * output), cv2.COLORMAP_JET)\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "    cam_img = heatmap + np.float32(image_tensor.cpu().squeeze().permute(1, 2, 0))\n",
        "    cam_img = cam_img / np.max(cam_img)\n",
        "\n",
        "    original_image = Image.open(image_path).convert('RGB')\n",
        "    original_image = transforms_valtest(original_image).numpy().transpose(1, 2, 0)\n",
        "\n",
        "    plt.imshow(cam_img)\n",
        "    plt.title(f'Predicted Class: {classes[class_idx]}')\n",
        "    plt.show()\n",
        "\n",
        "import random\n",
        "\n",
        "# 테스트 데이터셋에서 무작위로 이미지 선택\n",
        "random_index = random.randint(0, len(test_set) - 1)\n",
        "image_path = test_set.image_folder[random_index]\n",
        "image_path = os.path.join('/content/drive/MyDrive/datasets/test', image_path)\n",
        "\n",
        "# GradCAM 적용 함수 정의 (앞서 정의한 GradCAM 클래스와 apply_gradcam 함수)\n",
        "class_idx = test_set.__getitem__(random_index)[1]  # 이미지의 실제 클래스 인덱스\n",
        "target_layers = model.layer4[-1].to(DEVICE).conv2\n",
        "apply_gradcam(image_path, model, target_layers, classes)  # classes 변수에는 클래스 이름 목록을 할당"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1nfJHm8y2VeRSRJ1skZ0s9oUOeL_MnFSu",
      "authorship_tag": "ABX9TyM++YChp5BYmF7YjfHHoK5L",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}