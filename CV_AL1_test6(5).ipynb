{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yiyoungkim/yy/blob/main/CV_AL1_test6(5).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iaJLKOJG3nTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6da5c38c-c27d-42a6-9761-0550a400e9cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The goal is to solve the fine-grained image classification train the model using hyperparameter tuning, data augmentation, etc., and write a report with a detailed analysis of the results.\n",
        " - Is the problem solved in an appropriate way?\n",
        "- Are different hyperparameters and data augmentation used for training and are the results compared and analyzed?\n",
        "- Is the use of appropriate visualization tools and experiment tools for your reports?\n",
        "e.g., GradCAM, Weight & Biases, etc."
      ],
      "metadata": {
        "id": "PcT_ABspzPB7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9VcoyrJVwzlS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9a11de4-3f9c-4636-b003-95fba35142ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting adabelief-pytorch\n",
            "  Downloading adabelief_pytorch-0.2.1-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from adabelief-pytorch) (2.1.0+cu118)\n",
            "Collecting colorama>=0.4.0 (from adabelief-pytorch)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: tabulate>=0.7 in /usr/local/lib/python3.10/dist-packages (from adabelief-pytorch) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->adabelief-pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.0->adabelief-pytorch) (1.3.0)\n",
            "Installing collected packages: colorama, adabelief-pytorch\n",
            "Successfully installed adabelief-pytorch-0.2.1 colorama-0.4.6\n"
          ]
        }
      ],
      "source": [
        "!pip install adabelief-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZuomYJ5liLKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4778dd1-b8a5-4c1f-e03d-e267074b88d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import ToTensor,Normalize, RandomHorizontalFlip, Resize\n",
        "from torch.utils.data import ConcatDataset\n",
        "from torchvision.transforms import RandAugment\n",
        "\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from PIL import Image,ImageFilter,ImageEnhance\n",
        "\n",
        "from adabelief_pytorch import AdaBelief\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "from collections import Counter\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "\n",
        "# 랜덤 시드 설정\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(seed)\n",
        "\n",
        "### GPU Setting ###\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "print(DEVICE)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install wandb\n",
        "# import wandb\n",
        "# wandb.login()\n",
        "\n",
        "# # Initialize wandb\n",
        "# wandb.init(project=\"gpt5\")\n",
        "# config = wandb.config\n",
        "# config.dropout = 0.01"
      ],
      "metadata": {
        "id": "ZjNYzcBPPgTb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HamlO5MKZ-wk"
      },
      "outputs": [],
      "source": [
        "\n",
        "### Custom Dataset ###\n",
        "class CUB2011(Dataset):\n",
        "  def __init__(self, transform, mode='train'):\n",
        "    self.transform = transform\n",
        "    self.mode = mode\n",
        "\n",
        "    if self.mode == 'train':\n",
        "      self.image_folder = os.listdir('/content/drive/MyDrive/datasets/train')\n",
        "    elif self.mode == 'valid':\n",
        "      self.image_folder = os.listdir('/content/drive/MyDrive/datasets/valid')\n",
        "    elif self.mode == 'test':\n",
        "      self.image_folder = os.listdir('/content/drive/MyDrive/datasets/test')\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_folder)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = self.image_folder[idx]\n",
        "    img = Image.open(os.path.join('/content/drive/MyDrive/datasets', self.mode, img_path)).convert('RGB')\n",
        "    img = self.transform(img)\n",
        "\n",
        "\n",
        "    label = img_path.split('_')[-1].split('.')[0]\n",
        "    label = int(label)\n",
        "    return (img, label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "om7k9Cz5uytV"
      },
      "outputs": [],
      "source": [
        "# Geomentric transform + Visual corruptions\n",
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0, std=1, p=0.5):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if np.random.rand() < self.p:\n",
        "            img_array = np.array(img)\n",
        "            noise = np.random.normal(self.mean, self.std, img_array.shape)\n",
        "            noisy_image = np.clip(img_array + noise, 0, 255)  # Clip values to the range [0, 255]\n",
        "            return Image.fromarray(noisy_image.astype(np.uint8))\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std}, p={self.p})'\n",
        "\n",
        "\n",
        "class AdjustContrast(object):\n",
        "    def __init__(self, factor=1.0):\n",
        "        self.factor = factor\n",
        "\n",
        "    def __call__(self, img):\n",
        "        enhancer = ImageEnhance.Contrast(img)\n",
        "        img = enhancer.enhance(self.factor)\n",
        "        return img\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + f'(factor={self.factor})'\n",
        "\n",
        "class AdjustBrightness(object):\n",
        "    def __init__(self, factor=1.0):\n",
        "        self.factor = factor\n",
        "\n",
        "    def __call__(self, img):\n",
        "        enhancer = ImageEnhance.Brightness(img)\n",
        "        img = enhancer.enhance(self.factor)\n",
        "        return img\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + f'(factor={self.factor})'\n",
        "\n",
        "\n",
        "# Mix up\n",
        "def mixup_data(x, y, alpha=1.0):\n",
        "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size, device=x.device, dtype=torch.long)  # GPU에 있는 x.device를 사용하여 인덱스를 GPU로 전송\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c1qiR6PiqKL",
        "outputId": "ac08e538-9486-4d36-c136-89061c22ceb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of each dataset:  9440 296 298\n",
            "Loaded dataloader\n"
          ]
        }
      ],
      "source": [
        "### Data Preprocessing ###\n",
        "transforms_train_origin = transforms.Compose([transforms.Resize((448,448), Image.BICUBIC),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                                       ])\n",
        "transforms_test = transforms.Compose([transforms.Resize((448,448), Image.BICUBIC),\n",
        "                                       transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                                       ])\n",
        "\n",
        "# Apply RandAugment\n",
        "transforms_train_rand = transforms.Compose([\n",
        "    transforms.Resize((448, 448), Image.BICUBIC),\n",
        "    RandAugment(5,3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Geomentric transform + Visual corruptions\n",
        "transforms_train_g_v = transforms.Compose([\n",
        "    transforms.Resize((448, 448), Image.BICUBIC),\n",
        "    transforms.RandomResizedCrop(448),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.RandomVerticalFlip(0.5),\n",
        "    transforms.RandomRotation(30),\n",
        "    AddGaussianNoise(mean=0, std=25, p=0.5),  # 가우시안 노이즈를 추가합니다.\n",
        "    AdjustContrast(factor=2.0),  # 대비를 조절합니다.\n",
        "    AdjustBrightness(factor=1.5),  # 밝기를 조절합니다.\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_set_origin = CUB2011(mode='train',\n",
        "                    transform=transforms_train_origin)\n",
        "val_set = CUB2011(mode='valid',\n",
        "                  transform=transforms_test)\n",
        "test_set = CUB2011(mode='test',\n",
        "                  transform=transforms_test)\n",
        "\n",
        "# 데이터 증강을 위해 원래 데이터를 복사하고 추가\n",
        "train_set_augmented = CUB2011(mode='train', transform=transforms_train_rand)\n",
        "train_set_augmented2 = CUB2011(mode='train',transform=transforms_train_g_v)\n",
        "\n",
        "train_loader = DataLoader(train_set_origin,batch_size=BATCH_SIZE,shuffle=True)\n",
        "\n",
        "# Mixup된 이미지를 train_loader에 추가\n",
        "train_set_mixup = []\n",
        "for input, target in train_loader:\n",
        "    mixed_input, target_a, target_b, lam = mixup_data(input, target, alpha=1.0)\n",
        "    train_set_mixup.append((mixed_input, target_a, target_b, lam))\n",
        "train_set_mixup = transforms.Compose([transforms.Resize((448,448), Image.BICUBIC),\n",
        "                                       transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                                       ])\n",
        "train_set_mixup = CUB2011(mode='train', transform=train_set_mixup)\n",
        "\n",
        "# 각 클래스의 샘플 수를 계산\n",
        "class_counts = Counter([label for _, label in train_set_origin])\n",
        "\n",
        "# 가장 작은 클래스의 샘플 수를 찾아 minority_class_label로 지정\n",
        "minority_class_label = min(class_counts, key=class_counts.get)\n",
        "\n",
        "# Create a new dataset from replicated_data\n",
        "replicated_data = CUB2011(mode='train', transform=transforms_train_origin) #rand--> origin\n",
        "replicated_data.image_folder = [data for data in train_set_origin.image_folder if data[1] == minority_class_label]\n",
        "\n",
        "\n",
        "\n",
        "# 두 데이터셋을 연결하여 새로운 훈련 데이터셋 생성\n",
        "train_set_combined = ConcatDataset([train_set_origin, replicated_data,\n",
        "                                    train_set_augmented,train_set_augmented2,train_set_mixup])\n",
        "\n",
        "print('Num of each dataset: ',len(train_set_combined),len(val_set),len(test_set))\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "# Dataloader class는 bath기반의 딥러닝모델 학습을 위해서 mini batch를 만들어주는 역할을 한다\n",
        "# dataloader를 통해 dataset의 전체 데이터가 batch size로 나뉘게 된다\n",
        "# train_loader = DataLoader(train_set_combined,batch_size=BATCH_SIZE,shuffle=True)\n",
        "train_loader = DataLoader(train_set_combined, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_set,batch_size=BATCH_SIZE,shuffle=False)\n",
        "test_loader = DataLoader(test_set,batch_size=BATCH_SIZE,shuffle=False)\n",
        "\n",
        "print(\"Loaded dataloader\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BCFpbfxxlNe",
        "outputId": "88f876c3-e786-493f-b933-625e68ae8fc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: adabelief-pytorch in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from adabelief-pytorch) (2.1.0+cu118)\n",
            "Requirement already satisfied: colorama>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from adabelief-pytorch) (0.4.6)\n",
            "Requirement already satisfied: tabulate>=0.7 in /usr/local/lib/python3.10/dist-packages (from adabelief-pytorch) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->adabelief-pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.0->adabelief-pytorch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade adabelief-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1E8YNNbiwOC",
        "outputId": "7a0a0a3e-9b54-4127-9066-a4332af25d11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
            "\u001b[31mModifications to default arguments:\n",
            "\u001b[31m                           eps  weight_decouple    rectify\n",
            "-----------------------  -----  -----------------  ---------\n",
            "adabelief-pytorch=0.0.5  1e-08  False              False\n",
            ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
            "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
            "----------------------------------------------------------  ----------------------------------------------\n",
            "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
            "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
            "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
            "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
            "\u001b[0m\n",
            "Weight decoupling enabled in AdaBelief\n",
            "Rectification enabled in AdaBelief\n",
            "Created a learning model and optimizer\n"
          ]
        }
      ],
      "source": [
        "### Model / Optimizer ###\n",
        "EPOCH = 30\n",
        "\n",
        "lr = 0.0001\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "### Tranfer Learning ###\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_features,512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(512, 50)  # 클래스 수에 맞게 조정\n",
        ")\n",
        "model.to(DEVICE)\n",
        "\n",
        "#Weight Decay 적용: L2 regularization을 통해 과적합을 방지\n",
        "# optimizer = AdaBelief(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-16, weight_decouple=True, rectify=True)\n",
        "optimizer = AdaBelief(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-16, weight_decouple=True, rectify=True, weight_decay=1e-5)\n",
        "\n",
        "\n",
        "# 코사인 앤니얼링 스케줄러 초기화\n",
        "dataset_size = len(train_set_combined)  # 훈련 데이터셋 크기\n",
        "\n",
        "T_max = dataset_size / BATCH_SIZE  # 한 주기의 에폭 수\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=T_max, eta_min=0.001)\n",
        "\n",
        "print(\"Created a learning model and optimizer\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mTQsnbhbzm7C"
      },
      "outputs": [],
      "source": [
        "### Train/Evaluation ###\n",
        "def train(model,train_loader,optimizer,epoch):\n",
        "  model.train()\n",
        "  for i,(image,target) in enumerate(train_loader):\n",
        "    image,target = image.to(DEVICE),target.to(DEVICE)\n",
        "    output = model(image)\n",
        "    optimizer.zero_grad()\n",
        "    # loss func을 어떤 것을 사용할 것인지?\n",
        "    train_loss = F.cross_entropy(output,target).to(DEVICE)\n",
        "\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Implement gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.25)  # You can adjust the 'max_norm' parameter as needed\n",
        "\n",
        "    # 스케줄러 업데이트\n",
        "    scheduler.step()\n",
        "\n",
        "    if i%10 ==0:\n",
        "      print(\n",
        "          f'Train Epoch: {epoch} [{i}/{len(train_loader)}]\\tloss: {train_loss.item():6f}')\n",
        "\n",
        "  return train_loss\n",
        "\n",
        "def evaluate(model,val_loader):\n",
        "  model.eval()\n",
        "  eval_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for i,(image,target) in enumerate(val_loader):\n",
        "      image,target = image.to(DEVICE),target.to(DEVICE)\n",
        "      output = model(image)\n",
        "\n",
        "      eval_loss += F.cross_entropy(output,target, reduction='sum').item()\n",
        "      pred = output.max(1,keepdim=True)[1]\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  eval_loss /= len(val_loader.dataset)\n",
        "  eval_accuracy = 100*correct / len(val_loader.dataset)\n",
        "  return eval_loss,eval_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ2smo7ucLY0",
        "outputId": "98661b6e-500f-4b50-c635-9834ab6946d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/148]\tloss: 3.993910\n",
            "Train Epoch: 0 [10/148]\tloss: 3.963226\n",
            "Train Epoch: 0 [20/148]\tloss: 3.906213\n",
            "Train Epoch: 0 [30/148]\tloss: 3.977610\n",
            "Train Epoch: 0 [40/148]\tloss: 3.869851\n",
            "Train Epoch: 0 [50/148]\tloss: 3.879687\n",
            "Train Epoch: 0 [60/148]\tloss: 3.792749\n",
            "Train Epoch: 0 [70/148]\tloss: 3.771624\n",
            "Train Epoch: 0 [80/148]\tloss: 3.640353\n",
            "Train Epoch: 0 [90/148]\tloss: 3.210402\n",
            "Train Epoch: 0 [100/148]\tloss: 3.244782\n",
            "Train Epoch: 0 [110/148]\tloss: 3.060210\n",
            "Train Epoch: 0 [120/148]\tloss: 2.771820\n",
            "Train Epoch: 0 [130/148]\tloss: 2.523811\n",
            "Train Epoch: 0 [140/148]\tloss: 2.274426\n",
            "[0]Validation Loss: 1.9894,Accuracy: 59.7973%\n",
            "Train Epoch: 1 [0/148]\tloss: 2.032492\n",
            "Train Epoch: 1 [10/148]\tloss: 1.843138\n",
            "Train Epoch: 1 [20/148]\tloss: 1.590106\n",
            "Train Epoch: 1 [30/148]\tloss: 1.490438\n",
            "Train Epoch: 1 [40/148]\tloss: 1.415697\n",
            "Train Epoch: 1 [50/148]\tloss: 1.269517\n",
            "Train Epoch: 1 [60/148]\tloss: 1.214675\n",
            "Train Epoch: 1 [70/148]\tloss: 1.043219\n",
            "Train Epoch: 1 [80/148]\tloss: 1.001942\n",
            "Train Epoch: 1 [90/148]\tloss: 0.752665\n",
            "Train Epoch: 1 [100/148]\tloss: 0.820425\n",
            "Train Epoch: 1 [110/148]\tloss: 0.732443\n",
            "Train Epoch: 1 [120/148]\tloss: 0.992564\n",
            "Train Epoch: 1 [130/148]\tloss: 0.659489\n",
            "Train Epoch: 1 [140/148]\tloss: 0.854784\n",
            "[1]Validation Loss: 0.5743,Accuracy: 87.1622%\n",
            "Train Epoch: 2 [0/148]\tloss: 0.547039\n",
            "Train Epoch: 2 [10/148]\tloss: 0.888290\n",
            "Train Epoch: 2 [20/148]\tloss: 0.747803\n",
            "Train Epoch: 2 [30/148]\tloss: 0.607044\n",
            "Train Epoch: 2 [40/148]\tloss: 0.556675\n",
            "Train Epoch: 2 [50/148]\tloss: 0.741504\n",
            "Train Epoch: 2 [60/148]\tloss: 0.599397\n",
            "Train Epoch: 2 [70/148]\tloss: 0.716939\n",
            "Train Epoch: 2 [80/148]\tloss: 0.900370\n",
            "Train Epoch: 2 [90/148]\tloss: 0.693504\n",
            "Train Epoch: 2 [100/148]\tloss: 0.680756\n",
            "Train Epoch: 2 [110/148]\tloss: 0.682882\n",
            "Train Epoch: 2 [120/148]\tloss: 1.010635\n",
            "Train Epoch: 2 [130/148]\tloss: 0.673400\n",
            "Train Epoch: 2 [140/148]\tloss: 0.706560\n",
            "[2]Validation Loss: 1.1517,Accuracy: 66.2162%\n",
            "Train Epoch: 3 [0/148]\tloss: 0.608475\n",
            "Train Epoch: 3 [10/148]\tloss: 0.834025\n",
            "Train Epoch: 3 [20/148]\tloss: 0.799406\n",
            "Train Epoch: 3 [30/148]\tloss: 0.961687\n",
            "Train Epoch: 3 [40/148]\tloss: 0.694571\n",
            "Train Epoch: 3 [50/148]\tloss: 0.714115\n",
            "Train Epoch: 3 [60/148]\tloss: 0.527502\n",
            "Train Epoch: 3 [70/148]\tloss: 0.558287\n",
            "Train Epoch: 3 [80/148]\tloss: 0.526751\n",
            "Train Epoch: 3 [90/148]\tloss: 0.622824\n",
            "Train Epoch: 3 [100/148]\tloss: 0.616787\n",
            "Train Epoch: 3 [110/148]\tloss: 0.411350\n",
            "Train Epoch: 3 [120/148]\tloss: 0.586790\n",
            "Train Epoch: 3 [130/148]\tloss: 0.536597\n",
            "Train Epoch: 3 [140/148]\tloss: 0.475003\n",
            "[3]Validation Loss: 0.4543,Accuracy: 88.5135%\n",
            "Train Epoch: 4 [0/148]\tloss: 0.396001\n",
            "Train Epoch: 4 [10/148]\tloss: 0.519702\n",
            "Train Epoch: 4 [20/148]\tloss: 0.347918\n",
            "Train Epoch: 4 [30/148]\tloss: 0.262896\n",
            "Train Epoch: 4 [40/148]\tloss: 0.428194\n",
            "Train Epoch: 4 [50/148]\tloss: 0.273523\n",
            "Train Epoch: 4 [60/148]\tloss: 0.487657\n",
            "Train Epoch: 4 [70/148]\tloss: 0.370575\n",
            "Train Epoch: 4 [80/148]\tloss: 0.398838\n",
            "Train Epoch: 4 [90/148]\tloss: 0.504316\n",
            "Train Epoch: 4 [100/148]\tloss: 0.536154\n",
            "Train Epoch: 4 [110/148]\tloss: 0.355768\n",
            "Train Epoch: 4 [120/148]\tloss: 0.718098\n",
            "Train Epoch: 4 [130/148]\tloss: 0.600798\n",
            "Train Epoch: 4 [140/148]\tloss: 0.548433\n",
            "[4]Validation Loss: 2.2921,Accuracy: 50.3378%\n",
            "Train Epoch: 5 [0/148]\tloss: 0.789888\n",
            "Train Epoch: 5 [10/148]\tloss: 1.285584\n",
            "Train Epoch: 5 [20/148]\tloss: 0.610067\n",
            "Train Epoch: 5 [30/148]\tloss: 0.650226\n",
            "Train Epoch: 5 [40/148]\tloss: 0.819607\n",
            "Train Epoch: 5 [50/148]\tloss: 0.593274\n",
            "Train Epoch: 5 [60/148]\tloss: 0.616039\n",
            "Train Epoch: 5 [70/148]\tloss: 0.449418\n",
            "Train Epoch: 5 [80/148]\tloss: 0.493592\n",
            "Train Epoch: 5 [90/148]\tloss: 0.409458\n",
            "Train Epoch: 5 [100/148]\tloss: 0.321992\n",
            "Train Epoch: 5 [110/148]\tloss: 0.423607\n",
            "Train Epoch: 5 [120/148]\tloss: 0.460409\n",
            "Train Epoch: 5 [130/148]\tloss: 0.417421\n",
            "Train Epoch: 5 [140/148]\tloss: 0.404622\n",
            "[5]Validation Loss: 0.4678,Accuracy: 88.5135%\n",
            "Train Epoch: 6 [0/148]\tloss: 0.193614\n",
            "Train Epoch: 6 [10/148]\tloss: 0.236703\n",
            "Train Epoch: 6 [20/148]\tloss: 0.475422\n",
            "Train Epoch: 6 [30/148]\tloss: 0.168190\n",
            "Train Epoch: 6 [40/148]\tloss: 0.428696\n",
            "Train Epoch: 6 [50/148]\tloss: 0.499090\n",
            "Train Epoch: 6 [60/148]\tloss: 0.358714\n",
            "Train Epoch: 6 [70/148]\tloss: 0.258638\n",
            "Train Epoch: 6 [80/148]\tloss: 0.348666\n",
            "Train Epoch: 6 [90/148]\tloss: 0.506890\n",
            "Train Epoch: 6 [100/148]\tloss: 0.477975\n",
            "Train Epoch: 6 [110/148]\tloss: 0.718275\n",
            "Train Epoch: 6 [120/148]\tloss: 0.376328\n",
            "Train Epoch: 6 [130/148]\tloss: 0.366984\n",
            "Train Epoch: 6 [140/148]\tloss: 0.837277\n",
            "[6]Validation Loss: 1.1915,Accuracy: 66.5541%\n",
            "Train Epoch: 7 [0/148]\tloss: 0.624226\n",
            "Train Epoch: 7 [10/148]\tloss: 0.616625\n",
            "Train Epoch: 7 [20/148]\tloss: 0.500161\n",
            "Train Epoch: 7 [30/148]\tloss: 0.605348\n",
            "Train Epoch: 7 [40/148]\tloss: 0.715418\n",
            "Train Epoch: 7 [50/148]\tloss: 0.591374\n",
            "Train Epoch: 7 [60/148]\tloss: 0.460203\n",
            "Train Epoch: 7 [70/148]\tloss: 0.552866\n",
            "Train Epoch: 7 [80/148]\tloss: 0.508758\n",
            "Train Epoch: 7 [90/148]\tloss: 0.417515\n",
            "Train Epoch: 7 [100/148]\tloss: 0.576503\n",
            "Train Epoch: 7 [110/148]\tloss: 0.386232\n",
            "Train Epoch: 7 [120/148]\tloss: 0.378543\n",
            "Train Epoch: 7 [130/148]\tloss: 0.264586\n",
            "Train Epoch: 7 [140/148]\tloss: 0.357141\n",
            "[7]Validation Loss: 0.5144,Accuracy: 87.1622%\n",
            "Train Epoch: 8 [0/148]\tloss: 0.163840\n",
            "Train Epoch: 8 [10/148]\tloss: 0.225149\n",
            "Train Epoch: 8 [20/148]\tloss: 0.260342\n",
            "Train Epoch: 8 [30/148]\tloss: 0.265102\n",
            "Train Epoch: 8 [40/148]\tloss: 0.285016\n",
            "Train Epoch: 8 [50/148]\tloss: 0.391562\n",
            "Train Epoch: 8 [60/148]\tloss: 0.469265\n",
            "Train Epoch: 8 [70/148]\tloss: 0.324195\n",
            "Train Epoch: 8 [80/148]\tloss: 0.464865\n",
            "Train Epoch: 8 [90/148]\tloss: 0.709438\n",
            "Train Epoch: 8 [100/148]\tloss: 0.429449\n",
            "Train Epoch: 8 [110/148]\tloss: 0.548056\n",
            "Train Epoch: 8 [120/148]\tloss: 0.414073\n",
            "Train Epoch: 8 [130/148]\tloss: 0.397657\n",
            "Train Epoch: 8 [140/148]\tloss: 0.531734\n",
            "[8]Validation Loss: 1.9463,Accuracy: 55.0676%\n",
            "Train Epoch: 9 [0/148]\tloss: 0.422127\n",
            "Train Epoch: 9 [10/148]\tloss: 0.957176\n",
            "Train Epoch: 9 [20/148]\tloss: 0.891851\n",
            "Train Epoch: 9 [30/148]\tloss: 0.599088\n",
            "Train Epoch: 9 [40/148]\tloss: 0.490494\n",
            "Train Epoch: 9 [50/148]\tloss: 0.286661\n",
            "Train Epoch: 9 [60/148]\tloss: 0.426771\n",
            "Train Epoch: 9 [70/148]\tloss: 0.549362\n",
            "Train Epoch: 9 [80/148]\tloss: 0.472835\n",
            "Train Epoch: 9 [90/148]\tloss: 0.499000\n",
            "Train Epoch: 9 [100/148]\tloss: 0.206291\n",
            "Train Epoch: 9 [110/148]\tloss: 0.324876\n",
            "Train Epoch: 9 [120/148]\tloss: 0.373904\n",
            "Train Epoch: 9 [130/148]\tloss: 0.484487\n",
            "Train Epoch: 9 [140/148]\tloss: 0.483904\n",
            "[9]Validation Loss: 0.4991,Accuracy: 86.4865%\n",
            "Train Epoch: 10 [0/148]\tloss: 0.132549\n",
            "Train Epoch: 10 [10/148]\tloss: 0.320209\n",
            "Train Epoch: 10 [20/148]\tloss: 0.326543\n",
            "Train Epoch: 10 [30/148]\tloss: 0.212186\n",
            "Train Epoch: 10 [40/148]\tloss: 0.203133\n",
            "Train Epoch: 10 [50/148]\tloss: 0.265346\n",
            "Train Epoch: 10 [60/148]\tloss: 0.204210\n",
            "Train Epoch: 10 [70/148]\tloss: 0.122252\n",
            "Train Epoch: 10 [80/148]\tloss: 0.450774\n",
            "Train Epoch: 10 [90/148]\tloss: 0.499047\n",
            "Train Epoch: 10 [100/148]\tloss: 0.546155\n",
            "Train Epoch: 10 [110/148]\tloss: 0.785319\n",
            "Train Epoch: 10 [120/148]\tloss: 0.521179\n",
            "Train Epoch: 10 [130/148]\tloss: 0.619195\n",
            "Train Epoch: 10 [140/148]\tloss: 0.546807\n",
            "[10]Validation Loss: 1.6638,Accuracy: 61.1486%\n",
            "Train Epoch: 11 [0/148]\tloss: 0.559378\n",
            "Train Epoch: 11 [10/148]\tloss: 0.688295\n",
            "Train Epoch: 11 [20/148]\tloss: 0.762788\n",
            "Train Epoch: 11 [30/148]\tloss: 0.701211\n",
            "Train Epoch: 11 [40/148]\tloss: 0.763367\n",
            "Train Epoch: 11 [50/148]\tloss: 0.509772\n",
            "Train Epoch: 11 [60/148]\tloss: 0.639364\n",
            "Train Epoch: 11 [70/148]\tloss: 0.560980\n",
            "Train Epoch: 11 [80/148]\tloss: 0.284983\n",
            "Train Epoch: 11 [90/148]\tloss: 0.417490\n",
            "Train Epoch: 11 [100/148]\tloss: 0.471910\n",
            "Train Epoch: 11 [110/148]\tloss: 0.554603\n",
            "Train Epoch: 11 [120/148]\tloss: 0.344304\n",
            "Train Epoch: 11 [130/148]\tloss: 0.403818\n",
            "Train Epoch: 11 [140/148]\tloss: 0.388386\n",
            "[11]Validation Loss: 0.5153,Accuracy: 86.8243%\n",
            "Train Epoch: 12 [0/148]\tloss: 0.422398\n",
            "Train Epoch: 12 [10/148]\tloss: 0.212585\n",
            "Train Epoch: 12 [20/148]\tloss: 0.291017\n",
            "Train Epoch: 12 [30/148]\tloss: 0.124219\n",
            "Train Epoch: 12 [40/148]\tloss: 0.217818\n",
            "Train Epoch: 12 [50/148]\tloss: 0.455591\n",
            "Train Epoch: 12 [60/148]\tloss: 0.161596\n",
            "Train Epoch: 12 [70/148]\tloss: 0.555239\n",
            "Train Epoch: 12 [80/148]\tloss: 0.367677\n",
            "Train Epoch: 12 [90/148]\tloss: 0.316248\n",
            "Train Epoch: 12 [100/148]\tloss: 0.756878\n",
            "Train Epoch: 12 [110/148]\tloss: 0.479923\n",
            "Train Epoch: 12 [120/148]\tloss: 0.553513\n",
            "Train Epoch: 12 [130/148]\tloss: 0.721352\n",
            "Train Epoch: 12 [140/148]\tloss: 0.462759\n",
            "[12]Validation Loss: 2.1951,Accuracy: 60.4730%\n",
            "Train Epoch: 13 [0/148]\tloss: 0.593018\n",
            "Train Epoch: 13 [10/148]\tloss: 0.371658\n",
            "Train Epoch: 13 [20/148]\tloss: 0.547622\n",
            "Train Epoch: 13 [30/148]\tloss: 0.542251\n",
            "Train Epoch: 13 [40/148]\tloss: 0.344934\n",
            "Train Epoch: 13 [50/148]\tloss: 0.392673\n",
            "Train Epoch: 13 [60/148]\tloss: 0.386953\n",
            "Train Epoch: 13 [70/148]\tloss: 0.414634\n",
            "Train Epoch: 13 [80/148]\tloss: 0.305410\n",
            "Train Epoch: 13 [90/148]\tloss: 0.326980\n",
            "Train Epoch: 13 [100/148]\tloss: 0.607231\n",
            "Train Epoch: 13 [110/148]\tloss: 0.107787\n",
            "Train Epoch: 13 [120/148]\tloss: 0.465187\n",
            "Train Epoch: 13 [130/148]\tloss: 0.252857\n",
            "Train Epoch: 13 [140/148]\tloss: 0.479526\n",
            "[13]Validation Loss: 0.4591,Accuracy: 87.5000%\n",
            "Train Epoch: 14 [0/148]\tloss: 0.232596\n",
            "Train Epoch: 14 [10/148]\tloss: 0.329310\n",
            "Train Epoch: 14 [20/148]\tloss: 0.500826\n",
            "Train Epoch: 14 [30/148]\tloss: 0.241713\n",
            "Train Epoch: 14 [40/148]\tloss: 0.248984\n",
            "Train Epoch: 14 [50/148]\tloss: 0.285411\n",
            "Train Epoch: 14 [60/148]\tloss: 0.325360\n",
            "Train Epoch: 14 [70/148]\tloss: 0.251617\n",
            "Train Epoch: 14 [80/148]\tloss: 0.253245\n",
            "Train Epoch: 14 [90/148]\tloss: 0.355841\n",
            "Train Epoch: 14 [100/148]\tloss: 0.352567\n",
            "Train Epoch: 14 [110/148]\tloss: 0.950083\n",
            "Train Epoch: 14 [120/148]\tloss: 0.766295\n",
            "Train Epoch: 14 [130/148]\tloss: 0.851168\n",
            "Train Epoch: 14 [140/148]\tloss: 0.695970\n",
            "[14]Validation Loss: 1.1188,Accuracy: 72.6351%\n",
            "Train Epoch: 15 [0/148]\tloss: 0.464980\n",
            "Train Epoch: 15 [10/148]\tloss: 0.471666\n",
            "Train Epoch: 15 [20/148]\tloss: 0.571193\n",
            "Train Epoch: 15 [30/148]\tloss: 0.351945\n",
            "Train Epoch: 15 [40/148]\tloss: 0.429568\n",
            "Train Epoch: 15 [50/148]\tloss: 0.384328\n",
            "Train Epoch: 15 [60/148]\tloss: 0.624516\n",
            "Train Epoch: 15 [70/148]\tloss: 0.595787\n",
            "Train Epoch: 15 [80/148]\tloss: 0.633616\n",
            "Train Epoch: 15 [90/148]\tloss: 0.409910\n",
            "Train Epoch: 15 [100/148]\tloss: 0.332641\n",
            "Train Epoch: 15 [110/148]\tloss: 0.502023\n",
            "Train Epoch: 15 [120/148]\tloss: 0.434004\n",
            "Train Epoch: 15 [130/148]\tloss: 0.195239\n",
            "Train Epoch: 15 [140/148]\tloss: 0.234255\n",
            "[15]Validation Loss: 0.5131,Accuracy: 88.8514%\n",
            "Train Epoch: 16 [0/148]\tloss: 0.244548\n",
            "Train Epoch: 16 [10/148]\tloss: 0.183251\n",
            "Train Epoch: 16 [20/148]\tloss: 0.326721\n",
            "Train Epoch: 16 [30/148]\tloss: 0.409609\n",
            "Train Epoch: 16 [40/148]\tloss: 0.348688\n",
            "Train Epoch: 16 [50/148]\tloss: 0.238625\n",
            "Train Epoch: 16 [60/148]\tloss: 0.512148\n",
            "Train Epoch: 16 [70/148]\tloss: 0.203690\n",
            "Train Epoch: 16 [80/148]\tloss: 0.159782\n",
            "Train Epoch: 16 [90/148]\tloss: 0.461064\n",
            "Train Epoch: 16 [100/148]\tloss: 0.629416\n",
            "Train Epoch: 16 [110/148]\tloss: 0.582360\n",
            "Train Epoch: 16 [120/148]\tloss: 0.429506\n",
            "Train Epoch: 16 [130/148]\tloss: 0.706809\n",
            "Train Epoch: 16 [140/148]\tloss: 0.530805\n",
            "[16]Validation Loss: 1.2988,Accuracy: 66.2162%\n",
            "Train Epoch: 17 [0/148]\tloss: 0.517660\n",
            "Train Epoch: 17 [10/148]\tloss: 0.708346\n",
            "Train Epoch: 17 [20/148]\tloss: 0.530338\n",
            "Train Epoch: 17 [30/148]\tloss: 0.339869\n",
            "Train Epoch: 17 [40/148]\tloss: 0.635808\n",
            "Train Epoch: 17 [50/148]\tloss: 0.446035\n",
            "Train Epoch: 17 [60/148]\tloss: 0.458794\n",
            "Train Epoch: 17 [70/148]\tloss: 0.529414\n",
            "Train Epoch: 17 [80/148]\tloss: 0.419164\n",
            "Train Epoch: 17 [90/148]\tloss: 0.448150\n",
            "Train Epoch: 17 [100/148]\tloss: 0.347566\n",
            "Train Epoch: 17 [110/148]\tloss: 0.109740\n",
            "Train Epoch: 17 [120/148]\tloss: 0.220131\n",
            "Train Epoch: 17 [130/148]\tloss: 0.234095\n",
            "Train Epoch: 17 [140/148]\tloss: 0.268338\n",
            "[17]Validation Loss: 0.4682,Accuracy: 87.1622%\n",
            "Train Epoch: 18 [0/148]\tloss: 0.279964\n",
            "Train Epoch: 18 [10/148]\tloss: 0.330430\n",
            "Train Epoch: 18 [20/148]\tloss: 0.278027\n",
            "Train Epoch: 18 [30/148]\tloss: 0.245896\n",
            "Train Epoch: 18 [40/148]\tloss: 0.238960\n",
            "Train Epoch: 18 [50/148]\tloss: 0.257715\n",
            "Train Epoch: 18 [60/148]\tloss: 0.247465\n",
            "Train Epoch: 18 [70/148]\tloss: 0.250559\n",
            "Train Epoch: 18 [80/148]\tloss: 0.231425\n",
            "Train Epoch: 18 [90/148]\tloss: 0.105540\n",
            "Train Epoch: 18 [100/148]\tloss: 0.509909\n",
            "Train Epoch: 18 [110/148]\tloss: 0.356599\n",
            "Train Epoch: 18 [120/148]\tloss: 0.542559\n",
            "Train Epoch: 18 [130/148]\tloss: 0.366951\n",
            "Train Epoch: 18 [140/148]\tloss: 0.622197\n",
            "[18]Validation Loss: 0.9367,Accuracy: 76.0135%\n",
            "Train Epoch: 19 [0/148]\tloss: 0.516464\n",
            "Train Epoch: 19 [10/148]\tloss: 0.265116\n",
            "Train Epoch: 19 [20/148]\tloss: 0.591366\n",
            "Train Epoch: 19 [30/148]\tloss: 0.430795\n",
            "Train Epoch: 19 [40/148]\tloss: 0.426637\n",
            "Train Epoch: 19 [50/148]\tloss: 0.238413\n",
            "Train Epoch: 19 [60/148]\tloss: 0.561117\n",
            "Train Epoch: 19 [70/148]\tloss: 0.608138\n",
            "Train Epoch: 19 [80/148]\tloss: 0.568744\n",
            "Train Epoch: 19 [90/148]\tloss: 0.185269\n",
            "Train Epoch: 19 [100/148]\tloss: 0.388462\n",
            "Train Epoch: 19 [110/148]\tloss: 0.165869\n",
            "Train Epoch: 19 [120/148]\tloss: 0.223343\n",
            "Train Epoch: 19 [130/148]\tloss: 0.495589\n",
            "Train Epoch: 19 [140/148]\tloss: 0.215059\n",
            "[19]Validation Loss: 0.5754,Accuracy: 85.8108%\n",
            "Train Epoch: 20 [0/148]\tloss: 0.275437\n",
            "Train Epoch: 20 [10/148]\tloss: 0.230216\n",
            "Train Epoch: 20 [20/148]\tloss: 0.234799\n",
            "Train Epoch: 20 [30/148]\tloss: 0.016485\n",
            "Train Epoch: 20 [40/148]\tloss: 0.217979\n",
            "Train Epoch: 20 [50/148]\tloss: 0.229631\n",
            "Train Epoch: 20 [60/148]\tloss: 0.252177\n",
            "Train Epoch: 20 [70/148]\tloss: 0.148274\n",
            "Train Epoch: 20 [80/148]\tloss: 0.427159\n",
            "Train Epoch: 20 [90/148]\tloss: 0.228541\n",
            "Train Epoch: 20 [100/148]\tloss: 0.408684\n",
            "Train Epoch: 20 [110/148]\tloss: 0.521072\n",
            "Train Epoch: 20 [120/148]\tloss: 0.395827\n",
            "Train Epoch: 20 [130/148]\tloss: 0.641543\n",
            "Train Epoch: 20 [140/148]\tloss: 0.874488\n",
            "[20]Validation Loss: 1.1239,Accuracy: 75.6757%\n",
            "Train Epoch: 21 [0/148]\tloss: 0.492455\n",
            "Train Epoch: 21 [10/148]\tloss: 0.632834\n",
            "Train Epoch: 21 [20/148]\tloss: 0.208190\n",
            "Train Epoch: 21 [30/148]\tloss: 0.394779\n",
            "Train Epoch: 21 [40/148]\tloss: 0.813610\n",
            "Train Epoch: 21 [50/148]\tloss: 0.571664\n",
            "Train Epoch: 21 [60/148]\tloss: 0.466477\n",
            "Train Epoch: 21 [70/148]\tloss: 0.458582\n",
            "Train Epoch: 21 [80/148]\tloss: 0.599014\n",
            "Train Epoch: 21 [90/148]\tloss: 0.475975\n",
            "Train Epoch: 21 [100/148]\tloss: 0.136693\n",
            "Train Epoch: 21 [110/148]\tloss: 0.371611\n",
            "Train Epoch: 21 [120/148]\tloss: 0.345207\n",
            "Train Epoch: 21 [130/148]\tloss: 0.605339\n",
            "Train Epoch: 21 [140/148]\tloss: 0.454130\n",
            "[21]Validation Loss: 0.5051,Accuracy: 90.2027%\n",
            "Train Epoch: 22 [0/148]\tloss: 0.194043\n",
            "Train Epoch: 22 [10/148]\tloss: 0.375606\n",
            "Train Epoch: 22 [20/148]\tloss: 0.055158\n",
            "Train Epoch: 22 [30/148]\tloss: 0.393147\n",
            "Train Epoch: 22 [40/148]\tloss: 0.239883\n",
            "Train Epoch: 22 [50/148]\tloss: 0.259329\n",
            "Train Epoch: 22 [60/148]\tloss: 0.219186\n",
            "Train Epoch: 22 [70/148]\tloss: 0.732410\n",
            "Train Epoch: 22 [80/148]\tloss: 0.257080\n",
            "Train Epoch: 22 [90/148]\tloss: 0.262884\n",
            "Train Epoch: 22 [100/148]\tloss: 0.348493\n",
            "Train Epoch: 22 [110/148]\tloss: 0.739625\n",
            "Train Epoch: 22 [120/148]\tloss: 0.489071\n",
            "Train Epoch: 22 [130/148]\tloss: 0.564697\n",
            "Train Epoch: 22 [140/148]\tloss: 0.469517\n",
            "[22]Validation Loss: 1.3839,Accuracy: 69.2568%\n",
            "Train Epoch: 23 [0/148]\tloss: 0.614701\n",
            "Train Epoch: 23 [10/148]\tloss: 0.326789\n",
            "Train Epoch: 23 [20/148]\tloss: 0.404177\n",
            "Train Epoch: 23 [30/148]\tloss: 0.482824\n",
            "Train Epoch: 23 [40/148]\tloss: 0.333947\n",
            "Train Epoch: 23 [50/148]\tloss: 0.428734\n",
            "Train Epoch: 23 [60/148]\tloss: 0.506202\n",
            "Train Epoch: 23 [70/148]\tloss: 0.311575\n",
            "Train Epoch: 23 [80/148]\tloss: 0.444781\n",
            "Train Epoch: 23 [90/148]\tloss: 0.229627\n",
            "Train Epoch: 23 [100/148]\tloss: 0.384749\n",
            "Train Epoch: 23 [110/148]\tloss: 0.306716\n",
            "Train Epoch: 23 [120/148]\tloss: 0.214405\n",
            "Train Epoch: 23 [130/148]\tloss: 0.286804\n",
            "Train Epoch: 23 [140/148]\tloss: 0.281102\n",
            "[23]Validation Loss: 0.5702,Accuracy: 86.4865%\n",
            "Train Epoch: 24 [0/148]\tloss: 0.170570\n",
            "Train Epoch: 24 [10/148]\tloss: 0.146628\n",
            "Train Epoch: 24 [20/148]\tloss: 0.281578\n",
            "Train Epoch: 24 [30/148]\tloss: 0.199342\n",
            "Train Epoch: 24 [40/148]\tloss: 0.209180\n",
            "Train Epoch: 24 [50/148]\tloss: 0.269450\n",
            "Train Epoch: 24 [60/148]\tloss: 0.648062\n",
            "Train Epoch: 24 [70/148]\tloss: 0.358234\n",
            "Train Epoch: 24 [80/148]\tloss: 0.329098\n",
            "Train Epoch: 24 [90/148]\tloss: 0.411618\n",
            "Train Epoch: 24 [100/148]\tloss: 0.836190\n",
            "Train Epoch: 24 [110/148]\tloss: 0.398866\n",
            "Train Epoch: 24 [120/148]\tloss: 0.560858\n",
            "Train Epoch: 24 [130/148]\tloss: 0.690285\n",
            "Train Epoch: 24 [140/148]\tloss: 0.527560\n",
            "[24]Validation Loss: 1.2891,Accuracy: 68.2432%\n",
            "Train Epoch: 25 [0/148]\tloss: 0.425787\n",
            "Train Epoch: 25 [10/148]\tloss: 0.668035\n",
            "Train Epoch: 25 [20/148]\tloss: 0.392221\n",
            "Train Epoch: 25 [30/148]\tloss: 0.674284\n",
            "Train Epoch: 25 [40/148]\tloss: 0.459285\n",
            "Train Epoch: 25 [50/148]\tloss: 0.579695\n",
            "Train Epoch: 25 [60/148]\tloss: 0.449460\n",
            "Train Epoch: 25 [70/148]\tloss: 0.347553\n",
            "Train Epoch: 25 [80/148]\tloss: 0.036742\n",
            "Train Epoch: 25 [90/148]\tloss: 0.373005\n",
            "Train Epoch: 25 [100/148]\tloss: 0.411291\n",
            "Train Epoch: 25 [110/148]\tloss: 0.375257\n",
            "Train Epoch: 25 [120/148]\tloss: 0.256976\n",
            "Train Epoch: 25 [130/148]\tloss: 0.466067\n",
            "Train Epoch: 25 [140/148]\tloss: 0.317889\n",
            "[25]Validation Loss: 0.5313,Accuracy: 88.5135%\n",
            "Train Epoch: 26 [0/148]\tloss: 0.286415\n",
            "Train Epoch: 26 [10/148]\tloss: 0.197474\n",
            "Train Epoch: 26 [20/148]\tloss: 0.367215\n",
            "Train Epoch: 26 [30/148]\tloss: 0.282831\n",
            "Train Epoch: 26 [40/148]\tloss: 0.266319\n",
            "Train Epoch: 26 [50/148]\tloss: 0.336505\n",
            "Train Epoch: 26 [60/148]\tloss: 0.224745\n",
            "Train Epoch: 26 [70/148]\tloss: 0.450617\n",
            "Train Epoch: 26 [80/148]\tloss: 0.452549\n",
            "Train Epoch: 26 [90/148]\tloss: 0.255249\n",
            "Train Epoch: 26 [100/148]\tloss: 0.377338\n",
            "Train Epoch: 26 [110/148]\tloss: 0.333017\n",
            "Train Epoch: 26 [120/148]\tloss: 0.827097\n",
            "Train Epoch: 26 [130/148]\tloss: 0.328898\n",
            "Train Epoch: 26 [140/148]\tloss: 0.573369\n",
            "[26]Validation Loss: 1.3887,Accuracy: 71.2838%\n",
            "Train Epoch: 27 [0/148]\tloss: 0.342767\n",
            "Train Epoch: 27 [10/148]\tloss: 0.606757\n",
            "Train Epoch: 27 [20/148]\tloss: 0.204614\n",
            "Train Epoch: 27 [30/148]\tloss: 0.495676\n",
            "Train Epoch: 27 [40/148]\tloss: 0.536914\n",
            "Train Epoch: 27 [50/148]\tloss: 0.326789\n",
            "Train Epoch: 27 [60/148]\tloss: 0.656762\n",
            "Train Epoch: 27 [70/148]\tloss: 0.330999\n",
            "Train Epoch: 27 [80/148]\tloss: 0.318482\n",
            "Train Epoch: 27 [90/148]\tloss: 0.617343\n",
            "Train Epoch: 27 [100/148]\tloss: 0.366634\n",
            "Train Epoch: 27 [110/148]\tloss: 0.209913\n",
            "Train Epoch: 27 [120/148]\tloss: 0.344293\n",
            "Train Epoch: 27 [130/148]\tloss: 0.378363\n",
            "Train Epoch: 27 [140/148]\tloss: 0.343215\n",
            "[27]Validation Loss: 0.4979,Accuracy: 88.8514%\n",
            "Train Epoch: 28 [0/148]\tloss: 0.190783\n",
            "Train Epoch: 28 [10/148]\tloss: 0.344975\n",
            "Train Epoch: 28 [20/148]\tloss: 0.332812\n",
            "Train Epoch: 28 [30/148]\tloss: 0.356429\n",
            "Train Epoch: 28 [40/148]\tloss: 0.176957\n",
            "Train Epoch: 28 [50/148]\tloss: 0.242881\n",
            "Train Epoch: 28 [60/148]\tloss: 0.344332\n",
            "Train Epoch: 28 [70/148]\tloss: 0.292715\n",
            "Train Epoch: 28 [80/148]\tloss: 0.404694\n",
            "Train Epoch: 28 [90/148]\tloss: 0.392405\n",
            "Train Epoch: 28 [100/148]\tloss: 0.420314\n",
            "Train Epoch: 28 [110/148]\tloss: 0.524942\n",
            "Train Epoch: 28 [120/148]\tloss: 0.267501\n",
            "Train Epoch: 28 [130/148]\tloss: 0.483290\n",
            "Train Epoch: 28 [140/148]\tloss: 0.677020\n",
            "[28]Validation Loss: 0.9174,Accuracy: 78.0405%\n",
            "Train Epoch: 29 [0/148]\tloss: 0.578846\n",
            "Train Epoch: 29 [10/148]\tloss: 0.540159\n",
            "Train Epoch: 29 [20/148]\tloss: 0.649212\n",
            "Train Epoch: 29 [30/148]\tloss: 0.366662\n",
            "Train Epoch: 29 [40/148]\tloss: 0.291598\n",
            "Train Epoch: 29 [50/148]\tloss: 0.331703\n",
            "Train Epoch: 29 [60/148]\tloss: 0.327111\n",
            "Train Epoch: 29 [70/148]\tloss: 0.251928\n",
            "Train Epoch: 29 [80/148]\tloss: 0.242496\n",
            "Train Epoch: 29 [90/148]\tloss: 0.256256\n",
            "Train Epoch: 29 [100/148]\tloss: 0.256983\n",
            "Train Epoch: 29 [110/148]\tloss: 0.159496\n",
            "Train Epoch: 29 [120/148]\tloss: 0.335161\n",
            "Train Epoch: 29 [130/148]\tloss: 0.287802\n",
            "Train Epoch: 29 [140/148]\tloss: 0.271558\n",
            "[29]Validation Loss: 0.5266,Accuracy: 87.1622%\n",
            "[FINAL] Test Loss: 0.5512,Accuracy: 87.5839%\n",
            "Best Accuracy:  90.20270270270271\n",
            "Elasped Time: 2h, 126m, 53s\n",
            "time: 2h, 126m, 53s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#### Mian ###\n",
        "start = time.time()\n",
        "best = 0\n",
        "\n",
        "train_losses = []  # 훈련 손실을 저장할 목록\n",
        "val_losses = []    # 검증 손실을 저장할 목록\n",
        "val_accuracys = []\n",
        "\n",
        "# 디렉토리 생성\n",
        "os.makedirs('./best_model', exist_ok=True)\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "  train_loss = train(model,train_loader,optimizer,epoch)\n",
        "  val_loss,val_accuracy = evaluate(model,val_loader)\n",
        "\n",
        "  # # Log metrics to wandb\n",
        "  # wandb.log({\"loss\": loss.item()})\n",
        "\n",
        "  # 훈련 및 검증 손실을 목록에 추가\n",
        "  train_losses.append(train_loss)\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "  # Save best model\n",
        "  if val_accuracy > best:\n",
        "    best = val_accuracy\n",
        "    torch.save(model.state_dict(),\"./best_model.pth\")\n",
        "\n",
        "  val_accuracys.append(val_accuracy)\n",
        "  print(f\"[{epoch}]Validation Loss: {val_loss:.4f},Accuracy: {val_accuracy:.4f}%\")\n",
        "\n",
        "# Test result\n",
        "test_loss,test_accuracy = evaluate(model,test_loader)\n",
        "print(f'[FINAL] Test Loss: {test_loss:.4f},Accuracy: {test_accuracy:.4f}%')\n",
        "\n",
        "end = time.time()\n",
        "elasped_time = end - start\n",
        "\n",
        "print(\"Best Accuracy: \",best)\n",
        "print(\n",
        "    f\"Elasped Time: {int(elasped_time/3600)}h, {int(elasped_time/60)}m, {int(elasped_time%60)}s\")\n",
        "print(\n",
        "    f\"time: {int(elasped_time/3600)}h, {int(elasped_time/60)}m, {int(elasped_time%60)}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AtJ_lNacFqm"
      },
      "outputs": [],
      "source": [
        "# 이제 손실 값을 플로팅합니다.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# train_loss와 val_loss 텐서를 CPU로 이동하고 NumPy 배열로 변환\n",
        "train_losses = [loss.cpu().detach().numpy() if isinstance(loss, torch.Tensor) else loss for loss in train_losses]\n",
        "val_losses = [loss.\n",
        "              cpu().detach().numpy() if isinstance(loss, torch.Tensor) else loss for loss in val_losses]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, EPOCH + 1), train_losses, label='train_loss', marker='o')\n",
        "plt.plot(range(1, EPOCH + 1), val_losses, label='val_loss', marker='o')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "\n",
        "plt.xlabel('EPOCH')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "val_accuracys = [acc.cpu().detach().numpy() if isinstance(acc, torch.Tensor) else acc for acc in val_accuracys]\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, EPOCH + 1), val_accuracys, label='val_accuracy', marker='o')\n",
        "plt.title('Validation Accuracy Over Epochs')\n",
        "\n",
        "plt.xlabel('EPOCH')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbSDhRMYYvVy"
      },
      "outputs": [],
      "source": [
        "# !pip install git+https://github.com/jacobgil/pytorch-grad-cam.git\n",
        "\n",
        "# import copy\n",
        "# from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
        "# from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "# from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "# import numpy as np\n",
        "# from PIL import Image\n",
        "\n",
        "# import torchvision\n",
        "# #\n",
        "# # Pick up layers for visualization\n",
        "# target_layers = [model.layer4[-1]]\n",
        "# grad_cam = GradCam(model = model, target_layers, use_cuda=False)\n",
        "\n",
        "# # You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
        "# grayscale_cam = cam(input_tensor=input_tensor)\n",
        "\n",
        "# # In this example grayscale_cam has only one image in the batch:\n",
        "# grayscale_cam = grayscale_cam[0, :]\n",
        "# visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDscRu5Ndahh"
      },
      "outputs": [],
      "source": [
        "# # gradCAM\n",
        "# import cv2\n",
        "# from google.colab.patches import cv2_imshow                                      # cv2.imshow() should be replaced to cv2_imshow() in google.colab\n",
        "# import glob\n",
        "# import numpy as np\n",
        "\n",
        "\n",
        "# class GradCam(nn.Module):\n",
        "#     def __init__(self, model, module, layer):\n",
        "#         super().__init__()\n",
        "#         self.model = model\n",
        "#         self.module = module\n",
        "#         self.layer = layer\n",
        "#         self.register_hooks()\n",
        "\n",
        "#     def register_hooks(self):\n",
        "#         for modue_name, module in self.model._modules.items():\n",
        "#             if modue_name == self.module:\n",
        "#                 for layer_name, module in module._modules.items():\n",
        "#                     if layer_name == self.layer:\n",
        "#                         module.register_forward_hook(self.forward_hook)\n",
        "#                         module.register_backward_hook(self.backward_hook)\n",
        "\n",
        "#     def forward(self, input, target_index):\n",
        "#         outs = self.model(input)\n",
        "#         outs = outs.squeeze()                                                    # [1, num_classes]  --> [num_classes]\n",
        "#         outs = outs.to(DEVICE)\n",
        "\n",
        "#         # 가장 큰 값을 가지는 것을 target index 로 사용\n",
        "#         if target_index is None:\n",
        "#             target_index = outs.argmax()\n",
        "#             target_index = target_index.to(DEVICE)\n",
        "\n",
        "#         outs[target_index].backward(retain_graph=True)\n",
        "#         a_k = torch.mean(self.backward_result, dim=(1, 2), keepdim=True)         # [512, 1, 1]\n",
        "#         out = torch.sum(a_k * self.forward_result, dim=0).cuda()                 # [512, 7, 7] * [512, 1, 1]\n",
        "#         out = torch.relu(out) / torch.max(out)                                   # 음수를 없애고, 0 ~ 1 로 scaling # [7, 7]\n",
        "#         out = F.upsample_bilinear(out.unsqueeze(0).unsqueeze(0), [224, 224])     # 4D로 바꿈\n",
        "#         return out.cuda().detach().squeeze().numpy()\n",
        "\n",
        "#     def forward_hook(self, _, input, output):\n",
        "#         self.forward_result = torch.squeeze(output)\n",
        "\n",
        "#     def backward_hook(self, _, grad_input, grad_output):\n",
        "#         self.backward_result = torch.squeeze(grad_output[0])\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "\n",
        "#     def preprocess_image(img):\n",
        "#         means = [0.485, 0.456, 0.406]\n",
        "#         stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "#         preprocessed_img = img.copy()[:, :, ::-1]\n",
        "#         for i in range(3):\n",
        "#             preprocessed_img[:, :, i] = preprocessed_img[:, :, i] - means[i]\n",
        "#             preprocessed_img[:, :, i] = preprocessed_img[:, :, i] / stds[i]\n",
        "#         preprocessed_img = np.ascontiguousarray(np.transpose(preprocessed_img, (2, 0, 1)))\n",
        "#         preprocessed_img = torch.from_numpy(preprocessed_img)\n",
        "#         preprocessed_img.unsqueeze_(0)\n",
        "#         input = preprocessed_img.requires_grad_(True)\n",
        "#         return input\n",
        "\n",
        "\n",
        "#     def show_cam_on_image(img, mask):\n",
        "\n",
        "#         # mask = (np.max(mask) - np.min(mask)) / (mask - np.min(mask))\n",
        "#         heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
        "#         heatmap = np.float32(heatmap) / 255\n",
        "#         cam = heatmap + np.float32(img)\n",
        "#         cam = cam / np.max(cam)\n",
        "#         cv2_imshow(np.uint8(255 * cam))                                          # cv2_imshow()에서 두 개의 parameter 사용 불가 --> print 부분과 imshow 부분으로 분리\n",
        "#         print(\"cam\")\n",
        "#         cv2_imshow(np.uint8(heatmap * 255))\n",
        "#         print(\"heat map\\n\\n\\n\")\n",
        "#         cv2.waitKey()\n",
        "\n",
        "\n",
        "\n",
        "#     print(model)\n",
        "#     #model.eval()\n",
        "\n",
        "#     grad_cam = GradCam(model=model, module='features', layer='30')\n",
        "#     root = '/content/gdrive/MyDrive/CUB_200_2011_repackage_class50/datasets/test'\n",
        "#     img_list = os.listdir(root)\n",
        "#     img_list = sorted(glob.glob(os.path.join(root, '*.jpg')))\n",
        "#     i = 0                                                                        # just for numbering heat maps\n",
        "#     for img_path in img_list:\n",
        "#         img = cv2.imread(img_path, 1)\n",
        "#         #img = np.float32(cv2.resize(img, (224, 224))) / 255\n",
        "#         img = np.float32(cv2.resize(img, (448, 448))) / 255\n",
        "#         input = preprocess_image(img)\n",
        "#         input = input.to(DEVICE)                                                 # input datas should be on GPU\n",
        "#         mask = grad_cam(input, None)\n",
        "#         print(\"<{}'th image>\" .format(i))\n",
        "#         show_cam_on_image(img, mask)\n",
        "#         i = i + 1                                                                # update heat maps numbering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwwqjbtGZeGU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Display\n",
        "from IPython.display import Image, display\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "model_builder = keras.applications.xception.Xception\n",
        "img_size = (299, 299)\n",
        "preprocess_input = keras.applications.xception.preprocess_input\n",
        "decode_predictions = keras.applications.xception.decode_predictions\n",
        "\n",
        "last_conv_layer_name = \"block14_sepconv2_act\"\n",
        "\n",
        "# The local path to our target image\n",
        "import random\n",
        "\n",
        "# 테스트 데이터셋에서 무작위로 이미지 선택\n",
        "random_index = random.randint(0, len(test_set) - 1)\n",
        "img_path = test_set.image_folder[random_index]\n",
        "img_path = os.path.join('/content/drive/MyDrive/datasets/test', image_path)\n",
        "display(Image(img_path))\n",
        "\n",
        "def get_img_array(img_path, size):\n",
        "    # `img` is a PIL image of size 299x299\n",
        "    img = keras.utils.load_img(img_path, target_size=size)\n",
        "    # `array` is a float32 Numpy array of shape (299, 299, 3)\n",
        "    array = keras.utils.img_to_array(img)\n",
        "    # We add a dimension to transform our array into a \"batch\"\n",
        "    # of size (1, 299, 299, 3)\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "    return array\n",
        "\n",
        "\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
        "    # First, we create a model that maps the input image to the activations\n",
        "    # of the last conv layer as well as the output predictions\n",
        "    grad_model = keras.models.Model(\n",
        "        model.inputs, [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    # Then, we compute the gradient of the top predicted class for our input image\n",
        "    # with respect to the activations of the last conv layer\n",
        "    with tf.GradientTape() as tape:\n",
        "        last_conv_layer_output, preds = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(preds[0])\n",
        "        class_channel = preds[:, pred_index]\n",
        "\n",
        "    # This is the gradient of the output neuron (top predicted or chosen)\n",
        "    # with regard to the output feature map of the last conv layer\n",
        "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
        "\n",
        "    # This is a vector where each entry is the mean intensity of the gradient\n",
        "    # over a specific feature map channel\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # We multiply each channel in the feature map array\n",
        "    # by \"how important this channel is\" with regard to the top predicted class\n",
        "    # then sum all the channels to obtain the heatmap class activation\n",
        "    last_conv_layer_output = last_conv_layer_output[0]\n",
        "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "\n",
        "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "    #귀여워..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bB4foHegZuYI"
      },
      "outputs": [],
      "source": [
        "# Prepare image\n",
        "img_array = preprocess_input(get_img_array(img_path, size=img_size))\n",
        "\n",
        "# Make model\n",
        "model = model_builder(weights=\"imagenet\")\n",
        "\n",
        "# Remove last layer's softmax\n",
        "model.layers[-1].activation = None\n",
        "\n",
        "# Print what the top predicted class is\n",
        "preds = model.predict(img_array)\n",
        "print(\"Predicted:\", decode_predictions(preds, top=1)[0])\n",
        "\n",
        "# Generate class activation heatmap\n",
        "heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
        "\n",
        "# Display heatmap\n",
        "plt.matshow(heatmap)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6XMpS2aaCEq"
      },
      "outputs": [],
      "source": [
        "def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n",
        "    # Load the original image\n",
        "    img = keras.utils.load_img(img_path)\n",
        "    img = keras.utils.img_to_array(img)\n",
        "\n",
        "    # Rescale heatmap to a range 0-255\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "    # Use jet colormap to colorize heatmap\n",
        "    jet = cm.get_cmap(\"jet\")\n",
        "\n",
        "    # Use RGB values of the colormap\n",
        "    jet_colors = jet(np.arange(256))[:, :3]\n",
        "    jet_heatmap = jet_colors[heatmap]\n",
        "\n",
        "    # Create an image with RGB colorized heatmap\n",
        "    jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n",
        "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
        "    jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n",
        "\n",
        "    # Superimpose the heatmap on original image\n",
        "    superimposed_img = jet_heatmap * alpha + img\n",
        "    superimposed_img = keras.utils.array_to_img(superimposed_img)\n",
        "\n",
        "    # Save the superimposed image\n",
        "    superimposed_img.save(cam_path)\n",
        "\n",
        "    # Display Grad CAM\n",
        "    display(Image(cam_path))\n",
        "\n",
        "\n",
        "save_and_display_gradcam(img_path, heatmap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzF-6YiKlmu_"
      },
      "outputs": [],
      "source": [
        "#keras안쓰고 토치기반으로 하는 중인데 잘 안되구 오류나서 고치는 중입니다\n",
        "\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "\n",
        "    def save_gradient(self, grad):\n",
        "        self.gradients = grad\n",
        "\n",
        "    def save_activation(self, module, input, output):\n",
        "        self.activations = output\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def backward(self, output, class_idx):\n",
        "        self.model.zero_grad()\n",
        "        one_hot_output = torch.FloatTensor(1, output.size()[-1]).zero_().to(DEVICE)\n",
        "        one_hot_output[0][class_idx] = 1\n",
        "        one_hot_output = one_hot_output.to(DEVICE)\n",
        "\n",
        "        self.save_gradient(one_hot_output)\n",
        "        output.backward(gradient=one_hot_output, retain_graph=True)\n",
        "\n",
        "    def generate(self, input_image, class_idx):\n",
        "        self.model.eval()\n",
        "        hook = self.target_layer.register_forward_hook(self.save_activation)\n",
        "        output = self.forward(input_image)\n",
        "        hook.remove()\n",
        "\n",
        "        self.backward(output, class_idx)\n",
        "\n",
        "        gradients = self.gradients.cpu().data.numpy()[0]\n",
        "        activations = self.activations.cpu().data.numpy()[0]\n",
        "\n",
        "        weights = np.mean(gradients, axis=(1, 2))\n",
        "        cam = np.dot(activations.T, weights)\n",
        "        cam = np.maximum(0, cam)\n",
        "        cam = cv2.resize(cam, input_image.shape[2:])\n",
        "        cam = cam - np.min(cam)\n",
        "        cam = cam / np.max(cam)\n",
        "        return cam\n",
        "\n",
        "# Now, modify your apply_gradcam function as follows:\n",
        "\n",
        "def apply_gradcam(image_path, model, target_layer, classes):\n",
        "    model.eval()\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transforms_valtest(image).unsqueeze(0).to(DEVICE)\n",
        "    class_idx = model(image_tensor).max(1)[-1]\n",
        "\n",
        "    gradcam = GradCAM(model=model, target_layer=target_layer)\n",
        "    output = gradcam.generate(image_tensor, class_idx)\n",
        "\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * output), cv2.COLORMAP_JET)\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "    cam_img = heatmap + np.float32(image_tensor.cpu().squeeze().permute(1, 2, 0))\n",
        "    cam_img = cam_img / np.max(cam_img)\n",
        "\n",
        "    original_image = Image.open(image_path).convert('RGB')\n",
        "    original_image = transforms_valtest(original_image).numpy().transpose(1, 2, 0)\n",
        "\n",
        "    plt.imshow(cam_img)\n",
        "    plt.title(f'Predicted Class: {classes[class_idx]}')\n",
        "    plt.show()\n",
        "\n",
        "import random\n",
        "\n",
        "# 테스트 데이터셋에서 무작위로 이미지 선택\n",
        "random_index = random.randint(0, len(test_set) - 1)\n",
        "image_path = test_set.image_folder[random_index]\n",
        "image_path = os.path.join('/content/drive/MyDrive/datasets/test', image_path)\n",
        "\n",
        "# GradCAM 적용 함수 정의 (앞서 정의한 GradCAM 클래스와 apply_gradcam 함수)\n",
        "class_idx = test_set.__getitem__(random_index)[1]  # 이미지의 실제 클래스 인덱스\n",
        "target_layers = model.layer4[-1].to(DEVICE).conv2\n",
        "apply_gradcam(image_path, model, target_layers, classes)  # classes 변수에는 클래스 이름 목록을 할당"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}